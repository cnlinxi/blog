
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.3.3">
    
    
      
        <title>wenet总体解析 - 语音与语言处理</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.4a0965b7.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.cbb835fc.min.css">
        
      
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#wenet" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="语音与语言处理" class="md-header__button md-logo" aria-label="语音与语言处理" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            语音与语言处理
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              wenet总体解析
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/cnlinxi/blog" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    cnlinxi/blog
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        主页
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../paper/%E6%9C%AC%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB_2022_04_23/" class="md-tabs__link">
        最新论文
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../dsp/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/" class="md-tabs__link">
        信号处理
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../tts/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90%E6%A6%82%E8%BF%B0/" class="md-tabs__link">
        语音合成
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../%E5%8A%A0%E6%9D%83%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/" class="md-tabs__link md-tabs__link--active">
        语音识别
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../open_source_index/" class="md-tabs__link">
        开源数据和工具
      </a>
    </li>
  

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../../develop/docker/docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" class="md-tabs__link">
        开发
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../leetcode/滑动窗口专题.md" class="md-tabs__link">
        数据结构
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="语音与语言处理" class="md-nav__button md-logo" aria-label="语音与语言处理" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    语音与语言处理
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/cnlinxi/blog" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    cnlinxi/blog
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_1">
          主页
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="主页" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          主页
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        语音与语言处理
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          最新论文
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="最新论文" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          最新论文
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../paper/%E6%9C%AC%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB_2022_04_23/" class="md-nav__link">
        本周值得读-2022_04_23
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          信号处理
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="信号处理" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          信号处理
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../dsp/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/" class="md-nav__link">
        参考资料
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../dsp/%E8%AF%AD%E9%9F%B3%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/" class="md-nav__link">
        语音基本概念
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../dsp/%E8%AF%AD%E9%9F%B3%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/" class="md-nav__link">
        语音特征提取
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          语音合成
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="语音合成" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          语音合成
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tts/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90%E6%A6%82%E8%BF%B0/" class="md-nav__link">
        语音合成概述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tts/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90%E7%9A%84%E8%AF%84%E4%BB%B7%E6%A0%87%E5%87%86%E5%92%8C%E6%96%B9%E6%B3%95/" class="md-nav__link">
        语音合成的评价标准和方法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tts/%E8%AF%AD%E8%A8%80%E5%AD%A6/" class="md-nav__link">
        语言学
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tts/%E9%9F%B3%E5%BA%93%E5%88%B6%E4%BD%9C%E5%92%8C%E6%96%87%E6%9C%AC%E5%89%8D%E7%AB%AF/" class="md-nav__link">
        音库制作和文本前端
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tts/%E5%A3%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B/" class="md-nav__link">
        声学模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tts/%E5%A3%B0%E7%A0%81%E5%99%A8/" class="md-nav__link">
        声码器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../tts/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90%E7%9A%84%E6%80%BB%E4%BD%93%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/" class="md-nav__link">
        语音合成的总体知识体系
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          语音识别
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="语音识别" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          语音识别
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../%E5%8A%A0%E6%9D%83%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/" class="md-nav__link">
        加权有限状态机
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../arpa2fst%E5%B0%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%BD%AC%E4%B8%BAWFST/" class="md-nav__link">
        arpa2fst将语言模型转为WFST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../kaldi%E4%B8%AD%E7%9A%84%E8%A7%A3%E7%A0%81%E8%BF%87%E7%A8%8B/" class="md-nav__link">
        kaldi中的解码过程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../kaldi%E4%B8%AD%E7%9A%84Simple-Decoder/" class="md-nav__link">
        kaldi中的Simple-Decoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../OpenFST%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/" class="md-nav__link">
        OpenFST基本操作
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          wenet总体解析
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        wenet总体解析
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    发展历史
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    端到端识别基础
  </a>
  
    <nav class="md-nav" aria-label="端到端识别基础">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ctc" class="md-nav__link">
    CTC目标函数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention-based-encoder-decoder" class="md-nav__link">
    Attention-based Encoder Decoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#aed" class="md-nav__link">
    AED的解码
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    联合建模
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    降采样/降帧率
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    流式语音识别
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#wenet_1" class="md-nav__link">
    WeNet网络结构
  </a>
  
    <nav class="md-nav" aria-label="WeNet网络结构">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#encoder" class="md-nav__link">
    Encoder网络
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#subsampling" class="md-nav__link">
    编码器的SubSampling网络
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    上下文依赖
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#encoders" class="md-nav__link">
    编码器中的Encoders模块
  </a>
  
    <nav class="md-nav" aria-label="编码器中的Encoders模块">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#encoder_selfattn_layer" class="md-nav__link">
    encoder_selfattn_layer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#positionwise_layer" class="md-nav__link">
    positionwise_layer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convolution_layer" class="md-nav__link">
    convolution_layer
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention-based-decoder" class="md-nav__link">
    Attention based Decoder网络
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ctc-loss" class="md-nav__link">
    CTC Loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention-based-decoder-loss" class="md-nav__link">
    Attention-based Decoder Loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    整体前向过程
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#wenetmask" class="md-nav__link">
    WeNet的mask
  </a>
  
    <nav class="md-nav" aria-label="WeNet的mask">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batch-padding" class="md-nav__link">
    Batch Padding
  </a>
  
    <nav class="md-nav" aria-label="Batch Padding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attention-loss" class="md-nav__link">
    Attention Loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ctc-loss_1" class="md-nav__link">
    CTC Loss
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    自回归
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chunk-based-model" class="md-nav__link">
    Chunk-based Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mask" class="md-nav__link">
    编码器中的mask
  </a>
  
    <nav class="md-nav" aria-label="编码器中的mask">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#subsamplingmask" class="md-nav__link">
    下采样网络（Subsampling）中的mask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conformermask" class="md-nav__link">
    Conformer中卷积网络模块的mask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multiheadattentionmask" class="md-nav__link">
    MultiHeadAttention的mask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chunk-based-attention" class="md-nav__link">
    Chunk-based Attention
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mask_1" class="md-nav__link">
    解码器中的mask
  </a>
  
    <nav class="md-nav" aria-label="解码器中的mask">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#self-attention" class="md-nav__link">
    self-attention
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-attention" class="md-nav__link">
    cross-attention
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    整体结构
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    其它
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#wenetcache" class="md-nav__link">
    WeNet中的cache
  </a>
  
    <nav class="md-nav" aria-label="WeNet中的cache">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#runtime" class="md-nav__link">
    Runtime流式解码
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#python" class="md-nav__link">
    Python流式解码
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#baseencoderforward_chunk" class="md-nav__link">
    BaseEncoder.forward_chunk()分析
  </a>
  
    <nav class="md-nav" aria-label="BaseEncoder.forward_chunk()分析">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cache" class="md-nav__link">
    cache的大小
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offset" class="md-nav__link">
    offset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#subsampling_cache" class="md-nav__link">
    subsampling_cache
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#elayers_output_cache" class="md-nav__link">
    elayers_output_cache
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conformer_cnn_cache" class="md-nav__link">
    conformer_cnn_cache
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#wenet_2" class="md-nav__link">
    WeNet损失函数
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#wenet_3" class="md-nav__link">
    WeNet多机并行训练
  </a>
  
    <nav class="md-nav" aria-label="WeNet多机并行训练">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    业界现有方案
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributeddataparallel" class="md-nav__link">
    DistributedDataParallel
  </a>
  
    <nav class="md-nav" aria-label="DistributedDataParallel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pythonglobal-interpreter-lockgil" class="md-nav__link">
    Python全局解释器锁(Global Interpreter Lock，GIL)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dpdataparallel" class="md-nav__link">
    DP（DataParallel）
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ddpdp" class="md-nav__link">
    DDP为什么会比DP要快？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ddp" class="md-nav__link">
    DDP线性加速比
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ddp_1" class="md-nav__link">
    DDP中的概念
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ddp_2" class="md-nav__link">
    DDP的工作流程
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wenet_4" class="md-nav__link">
    WeNet分布式多机训练实现
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wenet_5" class="md-nav__link">
    WeNet分布式使用实践
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wenet_6" class="md-nav__link">
    WeNet实验结果
  </a>
  
    <nav class="md-nav" aria-label="WeNet实验结果">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#no_sync" class="md-nav__link">
    no_sync上下文
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    网络带宽对多机训练加速比的影响
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#no_sync_1" class="md-nav__link">
    no_sync上下文的影响
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    多机多卡的模型效果对比
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nfs" class="md-nav__link">
    基于NFS多机训练
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../wenet_runtime%E7%9F%A5%E8%AF%86%E7%82%B9/" class="md-nav__link">
        WeNet runtime知识点
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../wenet_kaitang-ssl-train%E7%9F%A5%E8%AF%86%E7%82%B9/" class="md-nav__link">
        wenet_kaitang-ssl-train知识点
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../wenet%E7%9A%84ctc_alignment/" class="md-nav__link">
        wenet的ctc_alignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../wenet%E7%9A%84ctc_prefix_beam_search/" class="md-nav__link">
        wenet的ctc_prefix_beam_search
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../wenet%E5%AF%B9onnx%E7%9A%84%E6%94%AF%E6%8C%81/" class="md-nav__link">
        wenet对onnx的支持
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../wenet%E4%B8%ADendpoint%E6%A3%80%E6%B5%8B/" class="md-nav__link">
        wenet中endpoint检测
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_6">
          开源数据和工具
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="开源数据和工具" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          开源数据和工具
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../open_source_index/" class="md-nav__link">
        主页
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7">
          开发
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="开发" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          开发
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_7_1" type="checkbox" id="__nav_7_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7_1">
          docker
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="docker" data-md-level="2">
        <label class="md-nav__title" for="__nav_7_1">
          <span class="md-nav__icon md-icon"></span>
          docker
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/docker/docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" class="md-nav__link">
        docker常用命令
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_7_2" type="checkbox" id="__nav_7_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7_2">
          python
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="python" data-md-level="2">
        <label class="md-nav__title" for="__nav_7_2">
          <span class="md-nav__icon md-icon"></span>
          python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/python/python%E7%9A%84%E5%8F%96%E5%8F%8D%E8%BF%90%E7%AE%97%E7%AC%A6/" class="md-nav__link">
        python的取反运算符
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/python/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/" class="md-nav__link">
        正则表达式
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/python/conda%E5%92%8Cpip/" class="md-nav__link">
        conda和pip
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/python/numpy%E5%AD%98%E5%8F%96%E6%95%B0%E6%8D%AE/" class="md-nav__link">
        numpy存取数据
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/python/__future__%E7%94%A8%E6%B3%95/" class="md-nav__link">
        __future__用法
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_7_3" type="checkbox" id="__nav_7_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7_3">
          shell
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="shell" data-md-level="2">
        <label class="md-nav__title" for="__nav_7_3">
          <span class="md-nav__icon md-icon"></span>
          shell
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/shell/Linux%E6%93%8D%E4%BD%9C%E5%9F%BA%E7%A1%80%E5%A4%87%E5%BF%98%E5%BD%95/" class="md-nav__link">
        Linux操作基础备忘录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/shell/vim/" class="md-nav__link">
        vim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/shell/top_linux%E4%B8%8B%E7%9A%84%E4%BB%BB%E5%8A%A1%E7%AE%A1%E7%90%86%E5%99%A8/" class="md-nav__link">
        top_linux下的任务管理器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/shell/shell%E8%B0%83%E8%AF%95/" class="md-nav__link">
        shell调试
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/shell/cat%E5%92%8CEOF/" class="md-nav__link">
        cat和EOF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/shell/tailf/" class="md-nav__link">
        tailf
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/shell/shell%E8%AF%AD%E6%B3%95%E9%80%9F%E6%9F%A5%E6%89%8B%E5%86%8C/" class="md-nav__link">
        shell语法速查手册
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_7_4" type="checkbox" id="__nav_7_4" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7_4">
          cpp
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="cpp" data-md-level="2">
        <label class="md-nav__title" for="__nav_7_4">
          <span class="md-nav__icon md-icon"></span>
          cpp
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/cpp/stl%E7%9A%84map/" class="md-nav__link">
        STL的map
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/cpp/cpp%E8%B0%83%E8%AF%95/" class="md-nav__link">
        cpp调试技巧
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/cpp/cpp%E7%9A%84%E7%B1%BB%E8%AE%BF%E9%97%AE%E4%BF%AE%E9%A5%B0%E7%AC%A6/" class="md-nav__link">
        cpp的类访问修饰符
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/cpp/cpp%E5%85%B3%E9%94%AE%E5%AD%97const%E4%BF%AE%E9%A5%B0%E5%87%BD%E6%95%B0/" class="md-nav__link">
        cpp关键字const修饰函数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/cpp/cpp%E5%AE%BD%E5%AD%97%E7%AC%A6wchar/" class="md-nav__link">
        cpp宽字符wchar
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/cpp/gcc%E7%9A%84%E5%B8%B8%E7%94%A8%E7%BC%96%E8%AF%91%E9%80%89%E9%A1%B9/" class="md-nav__link">
        GCC的常用编译选项
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/cpp/c%E6%88%96cpp%E8%BD%AF%E4%BB%B6%E7%BC%96%E8%AF%91/" class="md-nav__link">
        c或cpp软件编译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/cpp/stl%E7%9A%84vector/" class="md-nav__link">
        STL的vector
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/cpp/cpp%E4%B8%AD%E7%9A%84int_t/" class="md-nav__link">
        cpp中的int_t
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/cpp/stl%E7%9A%84string/" class="md-nav__link">
        string操作
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/cpp/cpp%E5%8F%B3%E5%80%BC%E5%BC%95%E7%94%A8/" class="md-nav__link">
        cpp右值引用和std::move
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/cpp/cpp%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/" class="md-nav__link">
        cpp并发编程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/cpp/c%E5%8F%8Acpp%E7%9A%84%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99/" class="md-nav__link">
        c及cpp的文件读写
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/cpp/cpp%E5%B8%B8%E7%94%A8%E5%BA%93gflags_glog_gtest/" class="md-nav__link">
        cpp常用库gflags/glog/gtest
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_7_5" type="checkbox" id="__nav_7_5" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7_5">
          git
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="git" data-md-level="2">
        <label class="md-nav__title" for="__nav_7_5">
          <span class="md-nav__icon md-icon"></span>
          git
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/git/git%E4%B8%AD%E7%9A%84HEAD/" class="md-nav__link">
        git中的HEAD
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/git/git%E5%B8%B8%E7%94%A8%E5%9B%9E%E9%80%80%E6%93%8D%E4%BD%9C/" class="md-nav__link">
        git常用回退操作
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/git/git%E7%9A%84%E6%8C%87%E9%92%88ref%E5%92%8Creflog/" class="md-nav__link">
        git的指针ref和reflog
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/git/git%E8%BD%AC%E7%A7%BB%E9%83%A8%E5%88%86%E4%BB%A3%E7%A0%81cherry-pick/" class="md-nav__link">
        git转移部分代码cherry-pick
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/git/git%E6%97%A5%E5%BF%97/" class="md-nav__link">
        git日志
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/git/git%E5%88%86%E6%94%AF%E5%90%88%E5%B9%B6/" class="md-nav__link">
        git分支合并
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/git/git%E7%9A%84fast_forward/" class="md-nav__link">
        Git的Fast Forward
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/git/git%E4%BF%AE%E6%94%B9%E6%8F%90%E4%BA%A4%E4%BF%A1%E6%81%AF/" class="md-nav__link">
        git修改提交信息
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../develop/git/git%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90/" class="md-nav__link">
        git学习资源
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_8" type="checkbox" id="__nav_8" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_8">
          数据结构
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="数据结构" data-md-level="1">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          数据结构
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../leetcode/滑动窗口专题.md" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../leetcode/DFS回溯算法专题.md" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../leetcode/一般二叉树专题.md" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../leetcode/二叉搜索树专题.md" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
  
                
  <a href="https://github.com/cnlinxi/blog/edit/main/docs/asr/wenet总体解析.md" title="编辑此页" class="md-content__button md-icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>



<h1 id="wenet">wenet总体解析</h1>
<h2 id="_1">发展历史</h2>
<ul>
<li>GMM-HMM（上世纪90年代）</li>
<li>深度学习爆发初期： DNN，CTC（2006）</li>
<li>RNN流行，Attention提出初期：RNN-T（2013），DeepSpeech（2014）， DeepSpeech2 (2016)， LAS（2016）</li>
<li>Attetion is all you need提出开始：Transformer（2017），Transformer-transducer（2020），Conformer（2020）</li>
</ul>
<p><img alt="" src="../attachments/Pasted%20image%2020220529180233.png" /></p>
<h2 id="_2">端到端识别基础</h2>
<p>一般在传统HMM框架下，会先利用HMM-GMM模型，通过对齐的方式，得到帧级别的对应标注，再通过帧级别损失函数来优化神经网络模型。如下是HMM-DNN模型的训练过程：</p>
<ol>
<li>对于每个句子扩展为单音素序列，用前向后向EM训练，得到单音素的hmm-单高斯model1。</li>
<li>用model1对句子做对齐，单高斯进行2倍分裂，更新模型，迭代这个对齐/分裂的过程n次，得到单音素的hmm-gmm模型model2。</li>
<li>用model2对句子做对齐，将音素根据上下文扩展为三音素，使用单高斯学习每个音素的决策树，最后每个叶子结点对应一个单高斯，得到一个三音素-hmm-单高斯模型model3。</li>
<li>类似于第2步，用model3不停迭代分裂高斯，得到三音素hmm-gmm的model4。</li>
<li>model4对句子做对齐，对齐数据用于帧级别NN训练。
...</li>
</ol>
<p>基于神经网络的端到端建模：</p>
<ol>
<li>直接以目标单元作为建模对象，比如中文使用<strong>字</strong>，英文使用<strong>字符</strong>或者<strong>BPE</strong>。</li>
<li>通过特殊的模型（目标函数），处理输入输出对齐未知的问题。</li>
</ol>
<p>这类端到端模型包括：</p>
<ol>
<li>基于CTC（Connectionist Temporal Classification）目标函数</li>
<li>AED（Attention-based Encoder Decoder）</li>
<li>RNN-T（Recurrent Neural Network Transducer）</li>
</ol>
<h3 id="ctc">CTC目标函数</h3>
<p>传统语音识别通过HMM来约束输出和输入的对齐方式，时间上保持单调，CTC是一种特殊的HMM约束。CTC本质上对所有合法的输出和输入对齐方式进行了穷举，所谓合法，即对齐后的输出序列能够按CTC规则规约得到原标注序列，则为合法对齐。</p>
<p>使用CTC目标函数会引入一个blank的输出单元，CTC对输出序列的规约规则为：</p>
<ul>
<li>对输出序列中连续的相同字符进行合并</li>
<li>移除输出序列中的blank字符</li>
</ul>
<p>举例来说，假设输入帧数为5帧，原标注序列为“我看看”，3个字，但是网络需要输出5个单元才能达到输入和输出一一对应。在CTC模型中，<em>通过对原标注中的3个单元进行重复或者插入blank来扩展到5个单元</em>。以下是两个可能的序列：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-1">1</a></span>
<span class="normal"><a href="#__codelineno-0-2">2</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1"></a>我看-看看
<a id="__codelineno-0-2" name="__codelineno-0-2"></a>我--看看
</code></pre></div></td></tr></table></div>
<p>其中，“-”表示blank，上述两个例子中，第一个对齐输出序列“我看-看看”是合法对齐序列，在规约之后能够获得原标注序列；而第二个对齐序列“我--看看”在规约之后是“我看”，不能获得原标注序列，因此不是合法对齐序列。当然除了“我看-看看”，还有非常多的合法序列，比如：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-1-1">1</a></span>
<span class="normal"><a href="#__codelineno-1-2">2</a></span>
<span class="normal"><a href="#__codelineno-1-3">3</a></span>
<span class="normal"><a href="#__codelineno-1-4">4</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1"></a>我看看-看
<a id="__codelineno-1-2" name="__codelineno-1-2"></a>我-看-看
<a id="__codelineno-1-3" name="__codelineno-1-3"></a>我看--看
<a id="__codelineno-1-4" name="__codelineno-1-4"></a>...
</code></pre></div></td></tr></table></div>
<p>CTC目标函数的思想是：既然不知道到底哪个对齐关系是正确的，那就<strong>最大化所有合法CTC对齐的概率之和</strong>。因此对于该例子，目标就是最大化如下概率：</p>
<div class="arithmatex">\[
P(我看看|X)=P(我看-看看|X)+P(我看看-看|X)+...+P(我看--看|X)
\]</div>
<p>求该目标函数的梯度，一种方式是穷举所有的有效CTC对齐，然后分别求梯度相加，但是这种方法的复杂度太高。由于CTC本身的结构特点，存在一种更为高效的动态规划算法，可以极大提升速度，参见：<a href="http://www.cs.toronto.edu/~graves/icml_2006.pdf">Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</a>，<a href="http://placebokkk.github.io/asr/2020/01/13/asr-ctc-eesen.html">Eesen中的CTC实现</a></p>
<p>解码时，模型对每一个输出帧都给出输出，这种解码方式称为<code>Frame同步</code>解码。如果某些帧输出为blank或者与前一帧是重复字符，则可以合并。由于穷举序列中blank所占比例较高，因此最终的模型倾向于输出尽可能少的非blank字符，解码序列中往往非blank字符只输出一个，这个称作CTC的<em>尖峰效应</em>。参见：</p>
<ul>
<li><a href="https://distill.pub/2017/ctc/">Sequence Modeling With CTC</a></li>
<li><a href="https://www.bilibili.com/video/BV1hZ4y1w7j1?p=4">李宏毅老师2020春课程-语音识别-语音合成-语音分离</a></li>
</ul>
<h3 id="attention-based-encoder-decoder">Attention-based Encoder Decoder</h3>
<p>AED（Attention-based Encoder Decoder），也称Seq2Seq框架，或者LAS（Listen, Attend and Spell）。模型中Encoder对输入序列（语音）进行编码，Decoder则是在目标序列（Decoder）上的自回归模型（输入之前的单元，预测下一个单元），在自回归计算中，通过Attention获取Encoder的编码信息，从而利用输入序列的信息。利用Attention学习输入输出间隐含的对齐适用于机器翻译这类任务，但对于语音识别或合成这类时序单调性任务，这种无约束反而会带来一些问题。</p>
<h3 id="aed">AED的解码</h3>
<p>解码时不对每一个输入帧都进行输出，而是根据整个输入序列信息和已输出信息进行下一次的输出，直到输出一个特殊结束字符，这种解码方法被称为<code>Label同步</code>解码。</p>
<p>CTC没有显式构建文本和语音之间的关系，RNN-T是一种显式建模了文本和语音之间关系的帧同步解码模型。标准AED中，编解码之间的Attention需要看到编码器的完整序列，因此无法进行流式识别，因此可以利用GMM-Attention、MoChA、MMA等单调递增的局部Attention方法进行改进。</p>
<h3 id="_3">联合建模</h3>
<p>联合使用CTC loss和AED可以有效加速模型收敛，同时可以得到更好的识别效果，目前该方法已经成为端到端学习的标准方法。在解码时，同时使用CTC和AED的输出，可以提高识别率。WeNet采用了先使用CTC解码，再用AED对CTC的Nbest结果进行Rescoring，这样即结合了两种模型的效果，又可以应用于流式场景。</p>
<h3 id="_4">降采样/降帧率</h3>
<p>输入序列越长，即帧的个数越多，网络计算量就越大。而在语音识别中，一定时间范围内的语音信号是相似的，多个连续帧对应的是同一个发音，另外，端到端语音识别使用建模单元一般是一个时间延续较长的单元（粗粒度），比如建模单元是一个中文汉字，假如一个汉字用时0.2s，0.2s对应20帧，如果将20帧的信息进行合并，比如合并为5帧，则可以线性的减少后续encoder网络的前向计算、CTC loss和AED计算cross attention时的开销。</p>
<p>可以用不同的神经网络来进行降采样，WeNet中使用的是2D-CNN。</p>
<h3 id="_5">流式语音识别</h3>
<p>CTC的输出相互独立，虽然CTC解码是<code>帧同步</code>的，但是要想</p>
<p>如果希望支持低延迟的流式识别，Encoder中的计算对右侧的依赖不能太长；此外，CTC可以进行帧同步解码，但CTC输出之间相互独立，使得每一帧利用上下文信息的能力不足，而基于Transformer、Conformer的自回归模型建模能力更强，但标准的Fully self-attention会对依赖整个序列，不能进行流式计算。</p>
<p>因此WeNet采用了<code>基于chunk的attention</code>，将序列划分为多个固定大小的chunk，每个chunk内部的帧不会依赖于chunk右侧的帧。同时，<em>连续堆叠的convolution层会带来较大的右侧依赖</em>，WeNet则采用了<code>因果卷积</code>来避免convolution层的右侧依赖：chunk attention+因果卷积。</p>
<p>此外，WeNet采用了CTC Nbest+Attention Rescoring的解码策略，首先利用CTC解码器输出Nbest的解码结果，然后利用Attention解码器对Nbest进行打分，输出得分最高的结果。</p>
<h2 id="wenet_1">WeNet网络结构</h2>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-2-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-2-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-2-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-2-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-2-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-2-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-2-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-2-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-2-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-2-10">10</a></span>
<span class="normal"><a href="#__codelineno-2-11">11</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1"></a><span class="k">class</span> <span class="nc">ASRModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a id="__codelineno-2-2" name="__codelineno-2-2"></a>    <span class="o">...</span>
<a id="__codelineno-2-3" name="__codelineno-2-3"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>  <span class="c1"># 编码器，包括下采样SubSampling网络和Transformer/Conformer模块</span>
<a id="__codelineno-2-4" name="__codelineno-2-4"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>  <span class="c1"># 解码器，cross-attention+self-attention</span>
<a id="__codelineno-2-5" name="__codelineno-2-5"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">ctc</span> <span class="o">=</span> <span class="n">ctc</span>  <span class="c1"># CTC loss</span>
<a id="__codelineno-2-6" name="__codelineno-2-6"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">criterion_att</span> <span class="o">=</span> <span class="n">LabelSmoothingLoss</span><span class="p">(</span>  <span class="c1"># Attention-based Decoder Loss</span>
<a id="__codelineno-2-7" name="__codelineno-2-7"></a>        <span class="n">size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
<a id="__codelineno-2-8" name="__codelineno-2-8"></a>        <span class="n">padding_idx</span><span class="o">=</span><span class="n">ignore_id</span><span class="p">,</span>
<a id="__codelineno-2-9" name="__codelineno-2-9"></a>        <span class="n">smoothing</span><span class="o">=</span><span class="n">lsm_weight</span><span class="p">,</span>
<a id="__codelineno-2-10" name="__codelineno-2-10"></a>        <span class="n">normalize_length</span><span class="o">=</span><span class="n">length_normalized_loss</span><span class="p">,</span>
<a id="__codelineno-2-11" name="__codelineno-2-11"></a>    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p><img alt="" src="../attachments/Pasted%20image%2020220529174915.png" /></p>
<p><code>ASRModel</code>的初始化函数中定义了encoder、decoder、ctc和criterion_att：</p>
<ul>
<li>encoder是Shared Encoder，其中包括了Subsampling网络</li>
<li>decoder是Attention-based Decoder网络</li>
<li>ctc是ctc Decoder网络（也就是前向网络和softmax）和ctc loss</li>
<li>criterion_att是attention-based decoder的自回归似然loss，实际是LabelSmoothing的loss。</li>
</ul>
<p><code>ASRModel</code>除了定义模型结构和实现前向计算之外，还有两个功能：</p>
<ul>
<li>
<p>提供多种python的解码接口</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-3-1">1</a></span>
<span class="normal"><a href="#__codelineno-3-2">2</a></span>
<span class="normal"><a href="#__codelineno-3-3">3</a></span>
<span class="normal"><a href="#__codelineno-3-4">4</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1"></a><span class="n">recognize</span><span class="p">()</span> <span class="c1"># attention decoder</span>
<a id="__codelineno-3-2" name="__codelineno-3-2"></a><span class="n">attention_rescoring</span><span class="p">()</span> <span class="c1"># CTC + attention rescoring</span>
<a id="__codelineno-3-3" name="__codelineno-3-3"></a><span class="n">ctc_prefix_beam_search</span><span class="p">()</span> <span class="c1"># CTC prefix beamsearch</span>
<a id="__codelineno-3-4" name="__codelineno-3-4"></a><span class="n">ctc_greedy_search</span><span class="p">()</span> <span class="c1"># CTC greedy search, reference: https://zhuanlan.zhihu.com/p/391848454</span>
</code></pre></div></td></tr></table></div>
</li>
<li>
<p>提供runtime中需要使用的接口，这些接口均有<code>@torch.jit.export</code>注解，可以在C++中调用
    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-4-1">1</a></span>
<span class="normal"><a href="#__codelineno-4-2">2</a></span>
<span class="normal"><a href="#__codelineno-4-3">3</a></span>
<span class="normal"><a href="#__codelineno-4-4">4</a></span>
<span class="normal"><a href="#__codelineno-4-5">5</a></span>
<span class="normal"><a href="#__codelineno-4-6">6</a></span>
<span class="normal"><a href="#__codelineno-4-7">7</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1"></a><span class="n">subsampling_rate</span><span class="p">()</span>
<a id="__codelineno-4-2" name="__codelineno-4-2"></a><span class="n">right_context</span><span class="p">()</span>
<a id="__codelineno-4-3" name="__codelineno-4-3"></a><span class="n">sos_symbol</span><span class="p">()</span>
<a id="__codelineno-4-4" name="__codelineno-4-4"></a><span class="n">eos_symbol</span><span class="p">()</span>
<a id="__codelineno-4-5" name="__codelineno-4-5"></a><span class="n">forward_encoder_chunk</span><span class="p">()</span>
<a id="__codelineno-4-6" name="__codelineno-4-6"></a><span class="n">forward_attention_decoder</span><span class="p">()</span>
<a id="__codelineno-4-7" name="__codelineno-4-7"></a><span class="n">ctc_activation</span><span class="p">()</span>
</code></pre></div></td></tr></table></div></p>
</li>
</ul>
<p>其中比较重要的是：</p>
<ul>
<li><code>forward_attention_decoder</code>: Attention Decoder的序列前向计算，非自回归模型</li>
<li><code>ctc_activation()</code>: CTC Decoder forward计算</li>
<li><code>forward_encoder_chunk()</code>: 基于chunk的Encoder forward计算</li>
</ul>
<h3 id="encoder">Encoder网络</h3>
<p>WeNet编码器支持Transformer和Conformer两种网络结构，<code>BaseEncoder</code>定义了如下统一的前向过程：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-5-1">1</a></span>
<span class="normal"><a href="#__codelineno-5-2">2</a></span>
<span class="normal"><a href="#__codelineno-5-3">3</a></span>
<span class="normal"><a href="#__codelineno-5-4">4</a></span>
<span class="normal"><a href="#__codelineno-5-5">5</a></span>
<span class="normal"><a href="#__codelineno-5-6">6</a></span>
<span class="normal"><a href="#__codelineno-5-7">7</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1"></a><span class="n">xs</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<a id="__codelineno-5-2" name="__codelineno-5-2"></a><span class="n">mask_pad</span> <span class="o">=</span> <span class="n">masks</span>  <span class="c1"># (B, 1, T/subsample_rate)</span>
<a id="__codelineno-5-3" name="__codelineno-5-3"></a><span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">add_optional_chunk_mask</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<a id="__codelineno-5-4" name="__codelineno-5-4"></a><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="p">:</span>
<a id="__codelineno-5-5" name="__codelineno-5-5"></a>    <span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">)</span>
<a id="__codelineno-5-6" name="__codelineno-5-6"></a><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
<a id="__codelineno-5-7" name="__codelineno-5-7"></a>    <span class="n">xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">after_norm</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>可以看到，Encoder由两部分组成：</p>
<ul>
<li><code>self.embed</code>是Subsampling网络</li>
<li><code>self.encoders</code>是一组相同网络结构的堆叠</li>
</ul>
<p>除了forward函数之外，Encoder还实现了两个方法：</p>
<ul>
<li><code>forward_chunk_by_chunk</code>，python解码时，模拟流式解码模式，基于chunk前向计算。</li>
<li><code>forward_chunk</code>，单次基于chunk的前向计算，通过ASRModel导出为<code>forward_encoder_chunk()</code>供runtime解码使用。</li>
</ul>
<h3 id="subsampling">编码器的SubSampling网络</h3>
<p>一方面为了降低计算复杂度，另一方面端到端ASR建模颗粒度较大，因此需要降采样。</p>
<p>语音中有两种使用CNN的方式，一种是2D-Conv，一种是1D-Conv：</p>
<ul>
<li>2D-Conv: 输入数据看作是深度(通道数）为1，高度为F（Fbank特征维度，idim），宽度为T（帧数）的一张图。</li>
<li>1D-Conv: 输入数据看作是深度(通道数）为F（Fbank特征维度)，高度为1，宽度为T（帧数）的一张图。</li>
</ul>
<p>Kaldi中著名的TDNN就是1D-Conv，在WeNet中采用2D-Conv实现降采样。WeNet中提供了多个降采样网络，例如将帧率降低4倍的网络<code>Conv2dSubsampling4</code>：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-6-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-6-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-6-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-6-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-6-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-6-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-6-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-6-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-6-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-6-10">10</a></span>
<span class="normal"><a href="#__codelineno-6-11">11</a></span>
<span class="normal"><a href="#__codelineno-6-12">12</a></span>
<span class="normal"><a href="#__codelineno-6-13">13</a></span>
<span class="normal"><a href="#__codelineno-6-14">14</a></span>
<span class="normal"><a href="#__codelineno-6-15">15</a></span>
<span class="normal"><a href="#__codelineno-6-16">16</a></span>
<span class="normal"><a href="#__codelineno-6-17">17</a></span>
<span class="normal"><a href="#__codelineno-6-18">18</a></span>
<span class="normal"><a href="#__codelineno-6-19">19</a></span>
<span class="normal"><a href="#__codelineno-6-20">20</a></span>
<span class="normal"><a href="#__codelineno-6-21">21</a></span>
<span class="normal"><a href="#__codelineno-6-22">22</a></span>
<span class="normal"><a href="#__codelineno-6-23">23</a></span>
<span class="normal"><a href="#__codelineno-6-24">24</a></span>
<span class="normal"><a href="#__codelineno-6-25">25</a></span>
<span class="normal"><a href="#__codelineno-6-26">26</a></span>
<span class="normal"><a href="#__codelineno-6-27">27</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1"></a><span class="k">class</span> <span class="nc">Conv2dSubsampling4</span><span class="p">(</span><span class="n">BaseSubsampling</span><span class="p">):</span>
<a id="__codelineno-6-2" name="__codelineno-6-2"></a>    <span class="sd">&quot;&quot;&quot;Convolutional 2D subsampling (to 1/4 length).</span>
<a id="__codelineno-6-3" name="__codelineno-6-3"></a>
<a id="__codelineno-6-4" name="__codelineno-6-4"></a><span class="sd">    Args:</span>
<a id="__codelineno-6-5" name="__codelineno-6-5"></a><span class="sd">        idim (int): Input dimension.</span>
<a id="__codelineno-6-6" name="__codelineno-6-6"></a><span class="sd">        odim (int): Output dimension.</span>
<a id="__codelineno-6-7" name="__codelineno-6-7"></a><span class="sd">        dropout_rate (float): Dropout rate.</span>
<a id="__codelineno-6-8" name="__codelineno-6-8"></a>
<a id="__codelineno-6-9" name="__codelineno-6-9"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-6-10" name="__codelineno-6-10"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">odim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<a id="__codelineno-6-11" name="__codelineno-6-11"></a>                 <span class="n">pos_enc_class</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a id="__codelineno-6-12" name="__codelineno-6-12"></a>        <span class="sd">&quot;&quot;&quot;Construct an Conv2dSubsampling4 object.&quot;&quot;&quot;</span>
<a id="__codelineno-6-13" name="__codelineno-6-13"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-6-14" name="__codelineno-6-14"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
<a id="__codelineno-6-15" name="__codelineno-6-15"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">odim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
<a id="__codelineno-6-16" name="__codelineno-6-16"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
<a id="__codelineno-6-17" name="__codelineno-6-17"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">odim</span><span class="p">,</span> <span class="n">odim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
<a id="__codelineno-6-18" name="__codelineno-6-18"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
<a id="__codelineno-6-19" name="__codelineno-6-19"></a>        <span class="p">)</span>
<a id="__codelineno-6-20" name="__codelineno-6-20"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
<a id="__codelineno-6-21" name="__codelineno-6-21"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">odim</span> <span class="o">*</span> <span class="p">(((</span><span class="n">idim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span> <span class="n">odim</span><span class="p">))</span>
<a id="__codelineno-6-22" name="__codelineno-6-22"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span> <span class="o">=</span> <span class="n">pos_enc_class</span>
<a id="__codelineno-6-23" name="__codelineno-6-23"></a>        <span class="c1"># The right context for every conv layer is computed by:</span>
<a id="__codelineno-6-24" name="__codelineno-6-24"></a>        <span class="c1"># (kernel_size - 1) * frame_rate_of_this_layer</span>
<a id="__codelineno-6-25" name="__codelineno-6-25"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">subsampling_rate</span> <span class="o">=</span> <span class="mi">4</span>
<a id="__codelineno-6-26" name="__codelineno-6-26"></a>        <span class="c1"># 6 = (3 - 1) * 1 + (3 - 1) * 2</span>
<a id="__codelineno-6-27" name="__codelineno-6-27"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">right_context</span> <span class="o">=</span> <span class="mi">6</span>
</code></pre></div></td></tr></table></div>
<p>这里利用Conv2D，将语音帧序列看做是一个时间轴为长，通道轴为宽，深度为1的图像，<code>Conv2dSubsampling4</code>通过两个<code>stride=2</code>的2d-CNN，将“图像”的长和宽都降为&frac14;。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-7-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-7-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-7-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-7-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-7-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-7-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-7-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-7-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-7-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-7-10">10</a></span>
<span class="normal"><a href="#__codelineno-7-11">11</a></span>
<span class="normal"><a href="#__codelineno-7-12">12</a></span>
<span class="normal"><a href="#__codelineno-7-13">13</a></span>
<span class="normal"><a href="#__codelineno-7-14">14</a></span>
<span class="normal"><a href="#__codelineno-7-15">15</a></span>
<span class="normal"><a href="#__codelineno-7-16">16</a></span>
<span class="normal"><a href="#__codelineno-7-17">17</a></span>
<span class="normal"><a href="#__codelineno-7-18">18</a></span>
<span class="normal"><a href="#__codelineno-7-19">19</a></span>
<span class="normal"><a href="#__codelineno-7-20">20</a></span>
<span class="normal"><a href="#__codelineno-7-21">21</a></span>
<span class="normal"><a href="#__codelineno-7-22">22</a></span>
<span class="normal"><a href="#__codelineno-7-23">23</a></span>
<span class="normal"><a href="#__codelineno-7-24">24</a></span>
<span class="normal"><a href="#__codelineno-7-25">25</a></span>
<span class="normal"><a href="#__codelineno-7-26">26</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1"></a><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
<a id="__codelineno-7-2" name="__codelineno-7-2"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-7-3" name="__codelineno-7-3"></a>        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-7-4" name="__codelineno-7-4"></a>        <span class="n">x_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-7-5" name="__codelineno-7-5"></a>        <span class="n">offset</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-7-6" name="__codelineno-7-6"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<a id="__codelineno-7-7" name="__codelineno-7-7"></a>    <span class="sd">&quot;&quot;&quot;Subsample x.</span>
<a id="__codelineno-7-8" name="__codelineno-7-8"></a>
<a id="__codelineno-7-9" name="__codelineno-7-9"></a><span class="sd">    Args:</span>
<a id="__codelineno-7-10" name="__codelineno-7-10"></a><span class="sd">        x (torch.Tensor): Input tensor (#batch, time, idim).</span>
<a id="__codelineno-7-11" name="__codelineno-7-11"></a><span class="sd">        x_mask (torch.Tensor): Input mask (#batch, 1, time).</span>
<a id="__codelineno-7-12" name="__codelineno-7-12"></a>
<a id="__codelineno-7-13" name="__codelineno-7-13"></a><span class="sd">    Returns:</span>
<a id="__codelineno-7-14" name="__codelineno-7-14"></a><span class="sd">        torch.Tensor: Subsampled tensor (#batch, time&#39;, odim),</span>
<a id="__codelineno-7-15" name="__codelineno-7-15"></a><span class="sd">            where time&#39; = time // 4.</span>
<a id="__codelineno-7-16" name="__codelineno-7-16"></a><span class="sd">        torch.Tensor: Subsampled mask (#batch, 1, time&#39;),</span>
<a id="__codelineno-7-17" name="__codelineno-7-17"></a><span class="sd">            where time&#39; = time // 4.</span>
<a id="__codelineno-7-18" name="__codelineno-7-18"></a><span class="sd">        torch.Tensor: positional encoding</span>
<a id="__codelineno-7-19" name="__codelineno-7-19"></a>
<a id="__codelineno-7-20" name="__codelineno-7-20"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-7-21" name="__codelineno-7-21"></a>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (b, c=1, t, f)</span>
<a id="__codelineno-7-22" name="__codelineno-7-22"></a>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-7-23" name="__codelineno-7-23"></a>    <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<a id="__codelineno-7-24" name="__codelineno-7-24"></a>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">c</span> <span class="o">*</span> <span class="n">f</span><span class="p">))</span>
<a id="__codelineno-7-25" name="__codelineno-7-25"></a>    <span class="n">x</span><span class="p">,</span> <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">offset</span><span class="p">)</span>
<a id="__codelineno-7-26" name="__codelineno-7-26"></a>    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">x_mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="mi">2</span><span class="p">][:,</span> <span class="p">:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
<p>注意：</p>
<ul>
<li>conv(x)中进行两次卷积，此时t维度<strong>约等于</strong>原来的&frac14;，因为没加padding，实际上是从长度T变为长度((T-1)/2-1)/2），注意经过卷积后深度不再是1。</li>
<li>pos_enc(x, offset) 经过subsampling之后，帧数变少了，此时再计算Positional Embedding。</li>
<li>x_mask是原始帧率下的记录batch各序列长度的mask，在计算attention以及ctc loss时均要使用，现在帧数降低了，x_mask也要跟着变化。</li>
</ul>
<p><img alt="" src="../attachments/Pasted%20image%2020220529174901.png" /></p>
<h3 id="_6">上下文依赖</h3>
<p>注意到Conv2dSubsampling4中的这两个变量：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-8-1">1</a></span>
<span class="normal"><a href="#__codelineno-8-2">2</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1"></a><span class="bp">self</span><span class="o">.</span><span class="n">subsampling_rate</span> <span class="o">=</span> <span class="mi">4</span>
<a id="__codelineno-8-2" name="__codelineno-8-2"></a><span class="bp">self</span><span class="o">.</span><span class="n">right_context</span> <span class="o">=</span> <span class="mi">6</span>
</code></pre></div></td></tr></table></div>
<p>这两个变量都在asr_model中进行了导出，在runtime时被使用。</p>
<p>在CTC或者WFST解码时，都是一帧一帧解码器，这里的帧指的是下采样（subsample）之后的帧，称之为<code>解码帧</code>，而模型输入的帧序列里面的帧（下采样之前的帧）称为<code>原始语音帧</code>。从上图可以看到：</p>
<ul>
<li>第1个解码帧，需要依赖第1到第7个原始语音帧。</li>
<li>第2个解码帧，需要依赖第5到第11个原始语音帧。</li>
</ul>
<p>那么上述两个变量</p>
<ul>
<li><code>subsampling_rate</code>指的就是对于相邻两个解码帧，在原始帧上的间隔。计算方法：<span class="arithmatex">\((kernel\_size-1)\times frame\_rate\_of\_this\_layer\)</span>，其中，<span class="arithmatex">\(frame\_rate\_of\_this\_layer\)</span>为一个下采样层的下采样倍率，上例中即为<span class="arithmatex">\((3-1)\times 2=4\)</span>。</li>
<li><code>right_context</code>指的是对于某个解码帧，其对应的第一个原始帧的右侧还需要额外依赖多少帧，才能获得这个解码帧的全部信息。计算方法：<span class="arithmatex">\(\sum_{n=1}^{N}(kernel\_size-1)\times n\)</span>，其中，<span class="arithmatex">\(N\)</span>为下采样层数，上例中即为<span class="arithmatex">\((3-1)\times 1+(3-1)\times 2=6\)</span>。</li>
</ul>
<p>在runtime解码器中，每次会送入一组帧进行前向计算并解码，一组（chunk）帧是定义在<strong>解码帧级别</strong>的。在处理第一个chunk时，接受输入获得当前chunk需要的所有的context，之后每次根据chunk大小和subsampling_rate获取新需要的原始帧。比如，chunk_size=1，则第一个chunk需要1-7帧（也即第一个chunk需要一个解码帧对应的所有原始帧），第二个chunk只要新拿到8-11帧即可（也即之后的chunk只需要<code>subsampling_rate</code>个原始帧）。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-9-1">1</a></span>
<span class="normal"><a href="#__codelineno-9-2">2</a></span>
<span class="normal"><a href="#__codelineno-9-3">3</a></span>
<span class="normal"><a href="#__codelineno-9-4">4</a></span>
<span class="normal"><a href="#__codelineno-9-5">5</a></span>
<span class="normal"><a href="#__codelineno-9-6">6</a></span>
<span class="normal"><a href="#__codelineno-9-7">7</a></span>
<span class="normal"><a href="#__codelineno-9-8">8</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1"></a><span class="c1">// runtime/core/decoder/asr_decoder.cc</span>
<a id="__codelineno-9-2" name="__codelineno-9-2"></a><span class="c1">// AsrModel::num_frames_for_chunk</span>
<a id="__codelineno-9-3" name="__codelineno-9-3"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">start</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">  </span><span class="c1">// First batch</span>
<a id="__codelineno-9-4" name="__codelineno-9-4"></a><span class="w">      </span><span class="kt">int</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">right_context_</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w">  </span><span class="c1">// Add current frame</span>
<a id="__codelineno-9-5" name="__codelineno-9-5"></a><span class="w">      </span><span class="n">num_requried_frames</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">chunk_size_</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">subsampling_rate_</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">context</span><span class="p">;</span><span class="w"></span>
<a id="__codelineno-9-6" name="__codelineno-9-6"></a><span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<a id="__codelineno-9-7" name="__codelineno-9-7"></a><span class="w">      </span><span class="n">num_requried_frames</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">chunk_size_</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">subsampling_rate_</span><span class="p">;</span><span class="w"></span>
<a id="__codelineno-9-8" name="__codelineno-9-8"></a><span class="w">    </span><span class="p">}</span><span class="w"></span>
</code></pre></div></td></tr></table></div>
<p>由于第二帧及其之后的chunk只需要<code>subsampling_rate</code>个原始帧，因此需要缓存前一个chunk的<code>1+right_context-subsampling_rate</code>个原始帧：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-10-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-10-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-10-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-10-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-10-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-10-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-10-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-10-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-10-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-10-10">10</a></span>
<span class="normal"><a href="#__codelineno-10-11">11</a></span>
<span class="normal"><a href="#__codelineno-10-12">12</a></span>
<span class="normal"><a href="#__codelineno-10-13">13</a></span>
<span class="normal"><a href="#__codelineno-10-14">14</a></span>
<span class="normal"><a href="#__codelineno-10-15">15</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1"></a><span class="c1">// runtime/core/decoder/asr_decoder.cc</span>
<a id="__codelineno-10-2" name="__codelineno-10-2"></a><span class="kt">void</span><span class="w"> </span><span class="nf">AsrModel::CacheFeature</span><span class="p">(</span><span class="w"></span>
<a id="__codelineno-10-3" name="__codelineno-10-3"></a><span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;&gt;&amp;</span><span class="w"> </span><span class="n">chunk_feats</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<a id="__codelineno-10-4" name="__codelineno-10-4"></a><span class="w">  </span><span class="c1">// Cache feature for next chunk</span>
<a id="__codelineno-10-5" name="__codelineno-10-5"></a><span class="w">  </span><span class="c1">// cached_feature_size为缓存大小</span>
<a id="__codelineno-10-6" name="__codelineno-10-6"></a><span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">cached_feature_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">right_context_</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">subsampling_rate_</span><span class="p">;</span><span class="w"></span>
<a id="__codelineno-10-7" name="__codelineno-10-7"></a><span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">chunk_feats</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="n">cached_feature_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<a id="__codelineno-10-8" name="__codelineno-10-8"></a><span class="w">    </span><span class="c1">// cached_feature_存放缓存数据</span>
<a id="__codelineno-10-9" name="__codelineno-10-9"></a><span class="w">    </span><span class="n">cached_feature_</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">cached_feature_size</span><span class="p">);</span><span class="w"></span>
<a id="__codelineno-10-10" name="__codelineno-10-10"></a><span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">cached_feature_size</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<a id="__codelineno-10-11" name="__codelineno-10-11"></a><span class="w">      </span><span class="n">cached_feature_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"></span>
<a id="__codelineno-10-12" name="__codelineno-10-12"></a><span class="w">        </span><span class="n">chunk_feats</span><span class="p">[</span><span class="n">chunk_feats</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">cached_feature_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">];</span><span class="w"></span>
<a id="__codelineno-10-13" name="__codelineno-10-13"></a><span class="w">    </span><span class="p">}</span><span class="w"></span>
<a id="__codelineno-10-14" name="__codelineno-10-14"></a><span class="w">  </span><span class="p">}</span><span class="w"></span>
<a id="__codelineno-10-15" name="__codelineno-10-15"></a><span class="p">}</span><span class="w"></span>
</code></pre></div></td></tr></table></div>
<h3 id="encoders">编码器中的Encoders模块</h3>
<p>对于编码器中的encoders模块，WeNet提供了Transformer和Conformer两种结构，均在wenet/transformer/encoder_layer.py中实现。</p>
<p>Transformer的self.encoders由一组TransformerEncoderLayer组成：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-11-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-11-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-11-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-11-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-11-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-11-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-11-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-11-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-11-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-11-10">10</a></span>
<span class="normal"><a href="#__codelineno-11-11">11</a></span>
<span class="normal"><a href="#__codelineno-11-12">12</a></span>
<span class="normal"><a href="#__codelineno-11-13">13</a></span>
<span class="normal"><a href="#__codelineno-11-14">14</a></span>
<span class="normal"><a href="#__codelineno-11-15">15</a></span>
<span class="normal"><a href="#__codelineno-11-16">16</a></span>
<span class="normal"><a href="#__codelineno-11-17">17</a></span>
<span class="normal"><a href="#__codelineno-11-18">18</a></span>
<span class="normal"><a href="#__codelineno-11-19">19</a></span>
<span class="normal"><a href="#__codelineno-11-20">20</a></span>
<span class="normal"><a href="#__codelineno-11-21">21</a></span>
<span class="normal"><a href="#__codelineno-11-22">22</a></span>
<span class="normal"><a href="#__codelineno-11-23">23</a></span>
<span class="normal"><a href="#__codelineno-11-24">24</a></span>
<span class="normal"><a href="#__codelineno-11-25">25</a></span>
<span class="normal"><a href="#__codelineno-11-26">26</a></span>
<span class="normal"><a href="#__codelineno-11-27">27</a></span>
<span class="normal"><a href="#__codelineno-11-28">28</a></span>
<span class="normal"><a href="#__codelineno-11-29">29</a></span>
<span class="normal"><a href="#__codelineno-11-30">30</a></span>
<span class="normal"><a href="#__codelineno-11-31">31</a></span>
<span class="normal"><a href="#__codelineno-11-32">32</a></span>
<span class="normal"><a href="#__codelineno-11-33">33</a></span>
<span class="normal"><a href="#__codelineno-11-34">34</a></span>
<span class="normal"><a href="#__codelineno-11-35">35</a></span>
<span class="normal"><a href="#__codelineno-11-36">36</a></span>
<span class="normal"><a href="#__codelineno-11-37">37</a></span>
<span class="normal"><a href="#__codelineno-11-38">38</a></span>
<span class="normal"><a href="#__codelineno-11-39">39</a></span>
<span class="normal"><a href="#__codelineno-11-40">40</a></span>
<span class="normal"><a href="#__codelineno-11-41">41</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1"></a><span class="k">class</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">BaseEncoder</span><span class="p">):</span>
<a id="__codelineno-11-2" name="__codelineno-11-2"></a>    <span class="sd">&quot;&quot;&quot;Transformer encoder module.&quot;&quot;&quot;</span>
<a id="__codelineno-11-3" name="__codelineno-11-3"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-11-4" name="__codelineno-11-4"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-11-5" name="__codelineno-11-5"></a>        <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<a id="__codelineno-11-6" name="__codelineno-11-6"></a>        <span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
<a id="__codelineno-11-7" name="__codelineno-11-7"></a>        <span class="n">attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
<a id="__codelineno-11-8" name="__codelineno-11-8"></a>        <span class="n">linear_units</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
<a id="__codelineno-11-9" name="__codelineno-11-9"></a>        <span class="n">num_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
<a id="__codelineno-11-10" name="__codelineno-11-10"></a>        <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
<a id="__codelineno-11-11" name="__codelineno-11-11"></a>        <span class="n">positional_dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
<a id="__codelineno-11-12" name="__codelineno-11-12"></a>        <span class="n">attention_dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
<a id="__codelineno-11-13" name="__codelineno-11-13"></a>        <span class="n">input_layer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;conv2d&quot;</span><span class="p">,</span>
<a id="__codelineno-11-14" name="__codelineno-11-14"></a>        <span class="n">pos_enc_layer_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;abs_pos&quot;</span><span class="p">,</span>
<a id="__codelineno-11-15" name="__codelineno-11-15"></a>        <span class="n">normalize_before</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-11-16" name="__codelineno-11-16"></a>        <span class="n">concat_after</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-11-17" name="__codelineno-11-17"></a>        <span class="n">static_chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<a id="__codelineno-11-18" name="__codelineno-11-18"></a>        <span class="n">use_dynamic_chunk</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-11-19" name="__codelineno-11-19"></a>        <span class="n">global_cmvn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-11-20" name="__codelineno-11-20"></a>        <span class="n">use_dynamic_left_chunk</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-11-21" name="__codelineno-11-21"></a>    <span class="p">):</span>
<a id="__codelineno-11-22" name="__codelineno-11-22"></a>        <span class="sd">&quot;&quot;&quot; Construct TransformerEncoder</span>
<a id="__codelineno-11-23" name="__codelineno-11-23"></a>
<a id="__codelineno-11-24" name="__codelineno-11-24"></a><span class="sd">        See Encoder for the meaning of each parameter.</span>
<a id="__codelineno-11-25" name="__codelineno-11-25"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-11-26" name="__codelineno-11-26"></a>        <span class="k">assert</span> <span class="n">check_argument_types</span><span class="p">()</span>
<a id="__codelineno-11-27" name="__codelineno-11-27"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">attention_heads</span><span class="p">,</span>
<a id="__codelineno-11-28" name="__codelineno-11-28"></a>                         <span class="n">linear_units</span><span class="p">,</span> <span class="n">num_blocks</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span>
<a id="__codelineno-11-29" name="__codelineno-11-29"></a>                         <span class="n">positional_dropout_rate</span><span class="p">,</span> <span class="n">attention_dropout_rate</span><span class="p">,</span>
<a id="__codelineno-11-30" name="__codelineno-11-30"></a>                         <span class="n">input_layer</span><span class="p">,</span> <span class="n">pos_enc_layer_type</span><span class="p">,</span> <span class="n">normalize_before</span><span class="p">,</span>
<a id="__codelineno-11-31" name="__codelineno-11-31"></a>                         <span class="n">concat_after</span><span class="p">,</span> <span class="n">static_chunk_size</span><span class="p">,</span> <span class="n">use_dynamic_chunk</span><span class="p">,</span>
<a id="__codelineno-11-32" name="__codelineno-11-32"></a>                         <span class="n">global_cmvn</span><span class="p">,</span> <span class="n">use_dynamic_left_chunk</span><span class="p">)</span>
<a id="__codelineno-11-33" name="__codelineno-11-33"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
<a id="__codelineno-11-34" name="__codelineno-11-34"></a>            <span class="n">TransformerEncoderLayer</span><span class="p">(</span>
<a id="__codelineno-11-35" name="__codelineno-11-35"></a>                <span class="n">output_size</span><span class="p">,</span>
<a id="__codelineno-11-36" name="__codelineno-11-36"></a>                <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">attention_heads</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>
<a id="__codelineno-11-37" name="__codelineno-11-37"></a>                                     <span class="n">attention_dropout_rate</span><span class="p">),</span>
<a id="__codelineno-11-38" name="__codelineno-11-38"></a>                <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">linear_units</span><span class="p">,</span>
<a id="__codelineno-11-39" name="__codelineno-11-39"></a>                                        <span class="n">dropout_rate</span><span class="p">),</span> <span class="n">dropout_rate</span><span class="p">,</span>
<a id="__codelineno-11-40" name="__codelineno-11-40"></a>                <span class="n">normalize_before</span><span class="p">,</span> <span class="n">concat_after</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">)</span>
<a id="__codelineno-11-41" name="__codelineno-11-41"></a>        <span class="p">])</span>
</code></pre></div></td></tr></table></div>
<p>Conformer的self.encoders由一组ConformerEncoderLayer组成：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-12-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-12-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-12-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-12-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-12-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-12-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-12-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-12-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-12-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-12-10">10</a></span>
<span class="normal"><a href="#__codelineno-12-11">11</a></span>
<span class="normal"><a href="#__codelineno-12-12">12</a></span>
<span class="normal"><a href="#__codelineno-12-13">13</a></span>
<span class="normal"><a href="#__codelineno-12-14">14</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1"></a><span class="bp">self</span><span class="o">.</span><span class="n">encoders</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
<a id="__codelineno-12-2" name="__codelineno-12-2"></a>    <span class="n">ConformerEncoderLayer</span><span class="p">(</span>
<a id="__codelineno-12-3" name="__codelineno-12-3"></a>        <span class="n">output_size</span><span class="p">,</span>
<a id="__codelineno-12-4" name="__codelineno-12-4"></a>        <span class="n">encoder_selfattn_layer</span><span class="p">(</span><span class="o">*</span><span class="n">encoder_selfattn_layer_args</span><span class="p">),</span>
<a id="__codelineno-12-5" name="__codelineno-12-5"></a>        <span class="n">positionwise_layer</span><span class="p">(</span><span class="o">*</span><span class="n">positionwise_layer_args</span><span class="p">),</span>
<a id="__codelineno-12-6" name="__codelineno-12-6"></a>        <span class="n">positionwise_layer</span><span class="p">(</span>
<a id="__codelineno-12-7" name="__codelineno-12-7"></a>            <span class="o">*</span><span class="n">positionwise_layer_args</span><span class="p">)</span> <span class="k">if</span> <span class="n">macaron_style</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-12-8" name="__codelineno-12-8"></a>        <span class="n">convolution_layer</span><span class="p">(</span>
<a id="__codelineno-12-9" name="__codelineno-12-9"></a>            <span class="o">*</span><span class="n">convolution_layer_args</span><span class="p">)</span> <span class="k">if</span> <span class="n">use_cnn_module</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-12-10" name="__codelineno-12-10"></a>        <span class="n">dropout_rate</span><span class="p">,</span>
<a id="__codelineno-12-11" name="__codelineno-12-11"></a>        <span class="n">normalize_before</span><span class="p">,</span>
<a id="__codelineno-12-12" name="__codelineno-12-12"></a>        <span class="n">concat_after</span><span class="p">,</span>
<a id="__codelineno-12-13" name="__codelineno-12-13"></a>    <span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">)</span>
<a id="__codelineno-12-14" name="__codelineno-12-14"></a><span class="p">])</span>
</code></pre></div></td></tr></table></div>
<p>Conformer原始论文中conformer block的结构如图：</p>
<p><img alt="" src="../attachments/Pasted%20image%2020220529174849.png" /></p>
<p>ConformerEncoderLayer涉及的主要模块有：</p>
<ul>
<li>encoder_selfattn_layer</li>
<li>positionwise_layer</li>
<li>convolution_layer</li>
</ul>
<p>如果不考虑cache，使用<code>normalize_before=True</code>,<code>feed_forward_macaron=True</code>，则WeNet中的ConformerEncoderLayer的forward可以简化为：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-13-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-13-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-13-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-13-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-13-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-13-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-13-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-13-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-13-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-13-10">10</a></span>
<span class="normal"><a href="#__codelineno-13-11">11</a></span>
<span class="normal"><a href="#__codelineno-13-12">12</a></span>
<span class="normal"><a href="#__codelineno-13-13">13</a></span>
<span class="normal"><a href="#__codelineno-13-14">14</a></span>
<span class="normal"><a href="#__codelineno-13-15">15</a></span>
<span class="normal"><a href="#__codelineno-13-16">16</a></span>
<span class="normal"><a href="#__codelineno-13-17">17</a></span>
<span class="normal"><a href="#__codelineno-13-18">18</a></span>
<span class="normal"><a href="#__codelineno-13-19">19</a></span>
<span class="normal"><a href="#__codelineno-13-20">20</a></span>
<span class="normal"><a href="#__codelineno-13-21">21</a></span>
<span class="normal"><a href="#__codelineno-13-22">22</a></span>
<span class="normal"><a href="#__codelineno-13-23">23</a></span>
<span class="normal"><a href="#__codelineno-13-24">24</a></span>
<span class="normal"><a href="#__codelineno-13-25">25</a></span>
<span class="normal"><a href="#__codelineno-13-26">26</a></span>
<span class="normal"><a href="#__codelineno-13-27">27</a></span>
<span class="normal"><a href="#__codelineno-13-28">28</a></span>
<span class="normal"><a href="#__codelineno-13-29">29</a></span>
<span class="normal"><a href="#__codelineno-13-30">30</a></span>
<span class="normal"><a href="#__codelineno-13-31">31</a></span>
<span class="normal"><a href="#__codelineno-13-32">32</a></span>
<span class="normal"><a href="#__codelineno-13-33">33</a></span>
<span class="normal"><a href="#__codelineno-13-34">34</a></span>
<span class="normal"><a href="#__codelineno-13-35">35</a></span>
<span class="normal"><a href="#__codelineno-13-36">36</a></span>
<span class="normal"><a href="#__codelineno-13-37">37</a></span>
<span class="normal"><a href="#__codelineno-13-38">38</a></span>
<span class="normal"><a href="#__codelineno-13-39">39</a></span>
<span class="normal"><a href="#__codelineno-13-40">40</a></span>
<span class="normal"><a href="#__codelineno-13-41">41</a></span>
<span class="normal"><a href="#__codelineno-13-42">42</a></span>
<span class="normal"><a href="#__codelineno-13-43">43</a></span>
<span class="normal"><a href="#__codelineno-13-44">44</a></span>
<span class="normal"><a href="#__codelineno-13-45">45</a></span>
<span class="normal"><a href="#__codelineno-13-46">46</a></span>
<span class="normal"><a href="#__codelineno-13-47">47</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1"></a><span class="k">class</span> <span class="nc">ConformerEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a id="__codelineno-13-2" name="__codelineno-13-2"></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
<a id="__codelineno-13-3" name="__codelineno-13-3"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-13-4" name="__codelineno-13-4"></a>        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-13-5" name="__codelineno-13-5"></a>        <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-13-6" name="__codelineno-13-6"></a>        <span class="n">pos_emb</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-13-7" name="__codelineno-13-7"></a>        <span class="n">mask_pad</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-13-8" name="__codelineno-13-8"></a>        <span class="n">output_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-13-9" name="__codelineno-13-9"></a>        <span class="n">cnn_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-13-10" name="__codelineno-13-10"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<a id="__codelineno-13-11" name="__codelineno-13-11"></a>        <span class="sd">&quot;&quot;&quot;Compute encoded features.</span>
<a id="__codelineno-13-12" name="__codelineno-13-12"></a>
<a id="__codelineno-13-13" name="__codelineno-13-13"></a><span class="sd">        Args:</span>
<a id="__codelineno-13-14" name="__codelineno-13-14"></a><span class="sd">            x (torch.Tensor): (#batch, time, size)</span>
<a id="__codelineno-13-15" name="__codelineno-13-15"></a><span class="sd">            mask (torch.Tensor): Mask tensor for the input (#batch, time，time).</span>
<a id="__codelineno-13-16" name="__codelineno-13-16"></a><span class="sd">            pos_emb (torch.Tensor): positional encoding, must not be None</span>
<a id="__codelineno-13-17" name="__codelineno-13-17"></a><span class="sd">                for ConformerEncoderLayer.</span>
<a id="__codelineno-13-18" name="__codelineno-13-18"></a><span class="sd">            mask_pad (torch.Tensor): batch padding mask used for conv module.</span>
<a id="__codelineno-13-19" name="__codelineno-13-19"></a><span class="sd">                (#batch, 1，time)</span>
<a id="__codelineno-13-20" name="__codelineno-13-20"></a><span class="sd">            output_cache (torch.Tensor): Cache tensor of the output</span>
<a id="__codelineno-13-21" name="__codelineno-13-21"></a><span class="sd">                (#batch, time2, size), time2 &lt; time in x.</span>
<a id="__codelineno-13-22" name="__codelineno-13-22"></a><span class="sd">            cnn_cache (torch.Tensor): Convolution cache in conformer layer</span>
<a id="__codelineno-13-23" name="__codelineno-13-23"></a><span class="sd">        Returns:</span>
<a id="__codelineno-13-24" name="__codelineno-13-24"></a><span class="sd">            torch.Tensor: Output tensor (#batch, time, size).</span>
<a id="__codelineno-13-25" name="__codelineno-13-25"></a><span class="sd">            torch.Tensor: Mask tensor (#batch, time).</span>
<a id="__codelineno-13-26" name="__codelineno-13-26"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-13-27" name="__codelineno-13-27"></a>
<a id="__codelineno-13-28" name="__codelineno-13-28"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff_macaron</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-13-29" name="__codelineno-13-29"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_macaron</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-13-30" name="__codelineno-13-30"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-13-31" name="__codelineno-13-31"></a>
<a id="__codelineno-13-32" name="__codelineno-13-32"></a>        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
<a id="__codelineno-13-33" name="__codelineno-13-33"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_mha</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-13-34" name="__codelineno-13-34"></a>        <span class="n">x_att</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<a id="__codelineno-13-35" name="__codelineno-13-35"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x_att</span><span class="p">)</span>
<a id="__codelineno-13-36" name="__codelineno-13-36"></a>
<a id="__codelineno-13-37" name="__codelineno-13-37"></a>        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
<a id="__codelineno-13-38" name="__codelineno-13-38"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-13-39" name="__codelineno-13-39"></a>        <span class="n">x</span><span class="p">,</span> <span class="n">new_cnn_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">,</span> <span class="n">cnn_cache</span><span class="p">)</span>
<a id="__codelineno-13-40" name="__codelineno-13-40"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-13-41" name="__codelineno-13-41"></a>
<a id="__codelineno-13-42" name="__codelineno-13-42"></a>        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
<a id="__codelineno-13-43" name="__codelineno-13-43"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-13-44" name="__codelineno-13-44"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-13-45" name="__codelineno-13-45"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-13-46" name="__codelineno-13-46"></a>
<a id="__codelineno-13-47" name="__codelineno-13-47"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>可以看到，对于encoder_selfattn_layer，positionwise_layer，convolution_layer，都是前有Layernorm，后有Dropout，再搭配残差Residual。</p>
<h4 id="encoder_selfattn_layer">encoder_selfattn_layer</h4>
<p>具体实现位于wenet/transformer/attention.py</p>
<p>attention.py中提供了两种attention的实现，<code>MultiHeadedAttention</code>和<code>RelPositionMultiHeadedAttention</code>。<code>MultiHeadedAttention</code>用于Transformer，<code>RelPositionMultiHeadedAttention</code>用于Conformer。原始的Conformer论文中提到的self-attention是Relative Position Multi Headed Attention，这是transformer-xl中提出的一种改进attention，和标准attention的区别在于，其中<strong>显式利用了相对位置信息</strong>，参见：<a href="https://zhuanlan.zhihu.com/p/344604604">Conformer ASR中的Relative Positional Embedding</a>。</p>
<h4 id="positionwise_layer">positionwise_layer</h4>
<p>具体实现位于wenet/transformer/positionwise_feed_forward.py</p>
<p>前向仿射变换，将<span class="arithmatex">\([B,T,H1]\)</span>变为<span class="arithmatex">\([B,T,H2]\)</span>，两个Linear，第一个后跟激活函数和Dropout，第二个直接输出。</p>
<h4 id="convolution_layer">convolution_layer</h4>
<p>具体实现位于wenet/transformer/convolution.py</p>
<p>WeNet采用了因果卷积（Causal Convolution），即不看右侧上下文，<em>这样无论模型含有多少卷积层，对右侧的上下文均没有依赖</em>。</p>
<p><img alt="" src="../attachments/Pasted%20image%2020220529174834.png" /></p>
<p>如上图左、中图所示，标准的卷积为了保证卷积后序列长度一致，需要在左右各pad长度为(kernel_size-1)//2的0.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-14-1">1</a></span>
<span class="normal"><a href="#__codelineno-14-2">2</a></span>
<span class="normal"><a href="#__codelineno-14-3">3</a></span>
<span class="normal"><a href="#__codelineno-14-4">4</a></span>
<span class="normal"><a href="#__codelineno-14-5">5</a></span>
<span class="normal"><a href="#__codelineno-14-6">6</a></span>
<span class="normal"><a href="#__codelineno-14-7">7</a></span>
<span class="normal"><a href="#__codelineno-14-8">8</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1"></a><span class="k">if</span> <span class="n">causal</span><span class="p">:</span> <span class="c1"># 使用因果卷积</span>
<a id="__codelineno-14-2" name="__codelineno-14-2"></a>    <span class="n">padding</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Conv1D函数设置的padding长度</span>
<a id="__codelineno-14-3" name="__codelineno-14-3"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># 因果卷积左侧手动padding的长度</span>
<a id="__codelineno-14-4" name="__codelineno-14-4"></a><span class="k">else</span><span class="p">:</span> <span class="c1"># 使用标准卷积</span>
<a id="__codelineno-14-5" name="__codelineno-14-5"></a>    <span class="c1"># kernel_size should be an odd number for none causal convolution</span>
<a id="__codelineno-14-6" name="__codelineno-14-6"></a>    <span class="k">assert</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span>
<a id="__codelineno-14-7" name="__codelineno-14-7"></a>    <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="c1"># Conv1D函数设置的padding长度</span>
<a id="__codelineno-14-8" name="__codelineno-14-8"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></td></tr></table></div>
<p>如上右图所示，因果卷积的实现很简单，只要在左侧pad长度为<code>kernel_size-1</code>的0，就可以实现因果卷积：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-15-1">1</a></span>
<span class="normal"><a href="#__codelineno-15-2">2</a></span>
<span class="normal"><a href="#__codelineno-15-3">3</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1"></a><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-15-2" name="__codelineno-15-2"></a>    <span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-15-3" name="__codelineno-15-3"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lorder</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<h3 id="attention-based-decoder">Attention based Decoder网络</h3>
<p>对于Attention based Decoder，WeNet提供了自回归Transformer和双向自回归Transformer结构，输入历史信息，输出当前识别结果，这种网络解码时只能依次产生输出，而不能一次产生整个输出序列。和Encoder中的attention层区别在于，Decoder除了self attention之外还要cross attention。</p>
<h3 id="ctc-loss">CTC Loss</h3>
<p>具体实现位于wenet/transformer/ctc.py</p>
<p>CTC Loss包含了<code>CTC decoder</code>和<code>CTC loss</code>两部分，CTC decoder仅仅对Encoder做一个前向线性变换，然后计算softmax：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-16-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-16-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-16-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-16-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-16-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-16-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-16-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-16-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-16-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-16-10">10</a></span>
<span class="normal"><a href="#__codelineno-16-11">11</a></span>
<span class="normal"><a href="#__codelineno-16-12">12</a></span>
<span class="normal"><a href="#__codelineno-16-13">13</a></span>
<span class="normal"><a href="#__codelineno-16-14">14</a></span>
<span class="normal"><a href="#__codelineno-16-15">15</a></span>
<span class="normal"><a href="#__codelineno-16-16">16</a></span>
<span class="normal"><a href="#__codelineno-16-17">17</a></span>
<span class="normal"><a href="#__codelineno-16-18">18</a></span>
<span class="normal"><a href="#__codelineno-16-19">19</a></span>
<span class="normal"><a href="#__codelineno-16-20">20</a></span>
<span class="normal"><a href="#__codelineno-16-21">21</a></span>
<span class="normal"><a href="#__codelineno-16-22">22</a></span>
<span class="normal"><a href="#__codelineno-16-23">23</a></span>
<span class="normal"><a href="#__codelineno-16-24">24</a></span>
<span class="normal"><a href="#__codelineno-16-25">25</a></span>
<span class="normal"><a href="#__codelineno-16-26">26</a></span>
<span class="normal"><a href="#__codelineno-16-27">27</a></span>
<span class="normal"><a href="#__codelineno-16-28">28</a></span>
<span class="normal"><a href="#__codelineno-16-29">29</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1"></a><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hs_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">hlens</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-16-2" name="__codelineno-16-2"></a>            <span class="n">ys_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ys_lens</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-16-3" name="__codelineno-16-3"></a>    <span class="sd">&quot;&quot;&quot;Calculate CTC loss.</span>
<a id="__codelineno-16-4" name="__codelineno-16-4"></a>
<a id="__codelineno-16-5" name="__codelineno-16-5"></a><span class="sd">    Args:</span>
<a id="__codelineno-16-6" name="__codelineno-16-6"></a><span class="sd">        hs_pad: batch of padded hidden state sequences (B, Tmax, D)</span>
<a id="__codelineno-16-7" name="__codelineno-16-7"></a><span class="sd">        hlens: batch of lengths of hidden state sequences (B)</span>
<a id="__codelineno-16-8" name="__codelineno-16-8"></a><span class="sd">        ys_pad: batch of padded character id sequence tensor (B, Lmax)</span>
<a id="__codelineno-16-9" name="__codelineno-16-9"></a><span class="sd">        ys_lens: batch of lengths of character sequence (B)</span>
<a id="__codelineno-16-10" name="__codelineno-16-10"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-16-11" name="__codelineno-16-11"></a>    <span class="c1"># hs_pad: (B, L, NProj) -&gt; ys_hat: (B, L, Nvocab)</span>
<a id="__codelineno-16-12" name="__codelineno-16-12"></a>    <span class="n">ys_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctc_lo</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hs_pad</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">))</span>
<a id="__codelineno-16-13" name="__codelineno-16-13"></a>    <span class="c1"># ys_hat: (B, L, D) -&gt; (L, B, D)</span>
<a id="__codelineno-16-14" name="__codelineno-16-14"></a>    <span class="n">ys_hat</span> <span class="o">=</span> <span class="n">ys_hat</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-16-15" name="__codelineno-16-15"></a>    <span class="n">ys_hat</span> <span class="o">=</span> <span class="n">ys_hat</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-16-16" name="__codelineno-16-16"></a>    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">ys_hat</span><span class="p">,</span> <span class="n">ys_pad</span><span class="p">,</span> <span class="n">hlens</span><span class="p">,</span> <span class="n">ys_lens</span><span class="p">)</span>
<a id="__codelineno-16-17" name="__codelineno-16-17"></a>    <span class="c1"># Batch-size average</span>
<a id="__codelineno-16-18" name="__codelineno-16-18"></a>    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">ys_hat</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-16-19" name="__codelineno-16-19"></a>    <span class="k">return</span> <span class="n">loss</span>
<a id="__codelineno-16-20" name="__codelineno-16-20"></a>
<a id="__codelineno-16-21" name="__codelineno-16-21"></a><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hs_pad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-16-22" name="__codelineno-16-22"></a>    <span class="sd">&quot;&quot;&quot;log_softmax of frame activations</span>
<a id="__codelineno-16-23" name="__codelineno-16-23"></a>
<a id="__codelineno-16-24" name="__codelineno-16-24"></a><span class="sd">    Args:</span>
<a id="__codelineno-16-25" name="__codelineno-16-25"></a><span class="sd">        Tensor hs_pad: 3d tensor (B, Tmax, eprojs)</span>
<a id="__codelineno-16-26" name="__codelineno-16-26"></a><span class="sd">    Returns:</span>
<a id="__codelineno-16-27" name="__codelineno-16-27"></a><span class="sd">        torch.Tensor: log softmax applied 3d tensor (B, Tmax, odim)</span>
<a id="__codelineno-16-28" name="__codelineno-16-28"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-16-29" name="__codelineno-16-29"></a>    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ctc_lo</span><span class="p">(</span><span class="n">hs_pad</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># F.log_softmax equivalent to log(softmax(x))</span>
</code></pre></div></td></tr></table></div>
<p>而CTC loss的部分则直接使用的torch提供的函数<code>torch.nn.CTCLoss</code>：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-17-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1"></a><span class="bp">self</span><span class="o">.</span><span class="n">ctc_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CTCLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction_type</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<h3 id="attention-based-decoder-loss">Attention-based Decoder Loss</h3>
<p>具体实现位于wenet/transformer/label_smoothing_loss.py</p>
<p>Attention-based Decoder的Loss是在最大化自回归的概率，在每个位置计算模型输出概率和样本标注概率的交叉熵，这个过程采用teacher forcing的方式，而不采用scheduled sampling。每个位置上，样本标注概率是一个one-hot的表示，既真实的标注概率为1，其他概率为0。Smoothing Loss中，对于样本标注概率，由原来的：</p>
<div class="arithmatex">\[
p_i=\left\{\begin{matrix}
1,\quad if\ i==y &amp; \\ 
0,\quad if \ i\neq y &amp; 
\end{matrix}\right.
\]</div>
<p>变为：</p>
<div class="arithmatex">\[
p_i=\left\{\begin{matrix}
1-\epsilon,\quad if\ i==y &amp; \\ 
\frac{\epsilon}{K-1},\quad if \ i\neq y &amp; 
\end{matrix}\right.
\]</div>
<p>上式中，<span class="arithmatex">\(K\)</span>表示类别数，<span class="arithmatex">\(\epsilon\)</span>表示极小的超参数。</p>
<h3 id="_7">整体前向过程</h3>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-18-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-18-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-18-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-18-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-18-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-18-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-18-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-18-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-18-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-18-10">10</a></span>
<span class="normal"><a href="#__codelineno-18-11">11</a></span>
<span class="normal"><a href="#__codelineno-18-12">12</a></span>
<span class="normal"><a href="#__codelineno-18-13">13</a></span>
<span class="normal"><a href="#__codelineno-18-14">14</a></span>
<span class="normal"><a href="#__codelineno-18-15">15</a></span>
<span class="normal"><a href="#__codelineno-18-16">16</a></span>
<span class="normal"><a href="#__codelineno-18-17">17</a></span>
<span class="normal"><a href="#__codelineno-18-18">18</a></span>
<span class="normal"><a href="#__codelineno-18-19">19</a></span>
<span class="normal"><a href="#__codelineno-18-20">20</a></span>
<span class="normal"><a href="#__codelineno-18-21">21</a></span>
<span class="normal"><a href="#__codelineno-18-22">22</a></span>
<span class="normal"><a href="#__codelineno-18-23">23</a></span>
<span class="normal"><a href="#__codelineno-18-24">24</a></span>
<span class="normal"><a href="#__codelineno-18-25">25</a></span>
<span class="normal"><a href="#__codelineno-18-26">26</a></span>
<span class="normal"><a href="#__codelineno-18-27">27</a></span>
<span class="normal"><a href="#__codelineno-18-28">28</a></span>
<span class="normal"><a href="#__codelineno-18-29">29</a></span>
<span class="normal"><a href="#__codelineno-18-30">30</a></span>
<span class="normal"><a href="#__codelineno-18-31">31</a></span>
<span class="normal"><a href="#__codelineno-18-32">32</a></span>
<span class="normal"><a href="#__codelineno-18-33">33</a></span>
<span class="normal"><a href="#__codelineno-18-34">34</a></span>
<span class="normal"><a href="#__codelineno-18-35">35</a></span>
<span class="normal"><a href="#__codelineno-18-36">36</a></span>
<span class="normal"><a href="#__codelineno-18-37">37</a></span>
<span class="normal"><a href="#__codelineno-18-38">38</a></span>
<span class="normal"><a href="#__codelineno-18-39">39</a></span>
<span class="normal"><a href="#__codelineno-18-40">40</a></span>
<span class="normal"><a href="#__codelineno-18-41">41</a></span>
<span class="normal"><a href="#__codelineno-18-42">42</a></span>
<span class="normal"><a href="#__codelineno-18-43">43</a></span>
<span class="normal"><a href="#__codelineno-18-44">44</a></span>
<span class="normal"><a href="#__codelineno-18-45">45</a></span>
<span class="normal"><a href="#__codelineno-18-46">46</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1"></a><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
<a id="__codelineno-18-2" name="__codelineno-18-2"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-18-3" name="__codelineno-18-3"></a>    <span class="n">speech</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-18-4" name="__codelineno-18-4"></a>    <span class="n">speech_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-18-5" name="__codelineno-18-5"></a>    <span class="n">text</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-18-6" name="__codelineno-18-6"></a>    <span class="n">text_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-18-7" name="__codelineno-18-7"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-18-8" name="__codelineno-18-8"></a>            <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<a id="__codelineno-18-9" name="__codelineno-18-9"></a>    <span class="sd">&quot;&quot;&quot;Frontend + Encoder + Decoder + Calc loss</span>
<a id="__codelineno-18-10" name="__codelineno-18-10"></a>
<a id="__codelineno-18-11" name="__codelineno-18-11"></a><span class="sd">    Args:</span>
<a id="__codelineno-18-12" name="__codelineno-18-12"></a><span class="sd">        speech: (Batch, Length, ...)</span>
<a id="__codelineno-18-13" name="__codelineno-18-13"></a><span class="sd">        speech_lengths: (Batch, )</span>
<a id="__codelineno-18-14" name="__codelineno-18-14"></a><span class="sd">        text: (Batch, Length)</span>
<a id="__codelineno-18-15" name="__codelineno-18-15"></a><span class="sd">        text_lengths: (Batch,)</span>
<a id="__codelineno-18-16" name="__codelineno-18-16"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-18-17" name="__codelineno-18-17"></a>    <span class="k">assert</span> <span class="n">text_lengths</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">text_lengths</span><span class="o">.</span><span class="n">shape</span>
<a id="__codelineno-18-18" name="__codelineno-18-18"></a>    <span class="c1"># Check that batch_size is unified</span>
<a id="__codelineno-18-19" name="__codelineno-18-19"></a>    <span class="k">assert</span> <span class="p">(</span><span class="n">speech</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">speech_lengths</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">text</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span>
<a id="__codelineno-18-20" name="__codelineno-18-20"></a>            <span class="n">text_lengths</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="p">(</span><span class="n">speech</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">speech_lengths</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
<a id="__codelineno-18-21" name="__codelineno-18-21"></a>                                        <span class="n">text</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">text_lengths</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<a id="__codelineno-18-22" name="__codelineno-18-22"></a>    <span class="c1"># 1. Encoder</span>
<a id="__codelineno-18-23" name="__codelineno-18-23"></a>    <span class="n">encoder_out</span><span class="p">,</span> <span class="n">encoder_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">speech</span><span class="p">,</span> <span class="n">speech_lengths</span><span class="p">)</span>
<a id="__codelineno-18-24" name="__codelineno-18-24"></a>    <span class="n">encoder_out_lens</span> <span class="o">=</span> <span class="n">encoder_mask</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-18-25" name="__codelineno-18-25"></a>
<a id="__codelineno-18-26" name="__codelineno-18-26"></a>    <span class="c1"># 2a. Attention-decoder branch</span>
<a id="__codelineno-18-27" name="__codelineno-18-27"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctc_weight</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
<a id="__codelineno-18-28" name="__codelineno-18-28"></a>        <span class="n">loss_att</span><span class="p">,</span> <span class="n">acc_att</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_att_loss</span><span class="p">(</span><span class="n">encoder_out</span><span class="p">,</span> <span class="n">encoder_mask</span><span class="p">,</span>
<a id="__codelineno-18-29" name="__codelineno-18-29"></a>                                                <span class="n">text</span><span class="p">,</span> <span class="n">text_lengths</span><span class="p">)</span>
<a id="__codelineno-18-30" name="__codelineno-18-30"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-18-31" name="__codelineno-18-31"></a>        <span class="n">loss_att</span> <span class="o">=</span> <span class="kc">None</span>
<a id="__codelineno-18-32" name="__codelineno-18-32"></a>
<a id="__codelineno-18-33" name="__codelineno-18-33"></a>    <span class="c1"># 2b. CTC branch</span>
<a id="__codelineno-18-34" name="__codelineno-18-34"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctc_weight</span> <span class="o">!=</span> <span class="mf">0.0</span><span class="p">:</span>
<a id="__codelineno-18-35" name="__codelineno-18-35"></a>        <span class="n">loss_ctc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctc</span><span class="p">(</span><span class="n">encoder_out</span><span class="p">,</span> <span class="n">encoder_out_lens</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span>
<a id="__codelineno-18-36" name="__codelineno-18-36"></a>                            <span class="n">text_lengths</span><span class="p">)</span>  <span class="c1"># CTCLoss(Log_probs,Targets,Input_lengths,Target_lengths)</span>
<a id="__codelineno-18-37" name="__codelineno-18-37"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-18-38" name="__codelineno-18-38"></a>        <span class="n">loss_ctc</span> <span class="o">=</span> <span class="kc">None</span>
<a id="__codelineno-18-39" name="__codelineno-18-39"></a>
<a id="__codelineno-18-40" name="__codelineno-18-40"></a>    <span class="k">if</span> <span class="n">loss_ctc</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-18-41" name="__codelineno-18-41"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_att</span>
<a id="__codelineno-18-42" name="__codelineno-18-42"></a>    <span class="k">elif</span> <span class="n">loss_att</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-18-43" name="__codelineno-18-43"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_ctc</span>
<a id="__codelineno-18-44" name="__codelineno-18-44"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-18-45" name="__codelineno-18-45"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctc_weight</span> <span class="o">*</span> <span class="n">loss_ctc</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctc_weight</span><span class="p">)</span> <span class="o">*</span> <span class="n">loss_att</span>
<a id="__codelineno-18-46" name="__codelineno-18-46"></a>    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">loss_att</span><span class="p">,</span> <span class="n">loss_ctc</span>
</code></pre></div></td></tr></table></div>
<h2 id="wenetmask">WeNet的mask</h2>
<p>mask是一个0、1值组成的掩码张量，WeNet里mask的语义为：mask中值为1的部分是需要考虑的，0的部分不考虑。WeNet的mask大致可以分为两类：</p>
<ul>
<li>序列mask，[Batch,Length]，每个[Length,]中值为1的位置代表了本序列需要考虑的部分。</li>
<li>Attention mask，[Batch,L1,L2]，每个[L1,L2]用于约束L1中哪些位置只能对L2中哪些位置进行attention操作。</li>
</ul>
<h3 id="batch-padding">Batch Padding</h3>
<p>不定长序列填充到等长tensor。WeNet中，输入的padding叫做<code>frame batch padding</code>，标注的padding叫<code>label batch padding</code>。</p>
<h4 id="attention-loss">Attention Loss</h4>
<p>利用一个特殊数值padding_idx将原先不等长的目标序列填充到等长序列，在计算Attention Loss时，对于值为padding_idx的目标，不参与loss计算。具体实现位于wenet/transformer/label_smoothing_loss.py</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-19-1">1</a></span>
<span class="normal"><a href="#__codelineno-19-2">2</a></span>
<span class="normal"><a href="#__codelineno-19-3">3</a></span>
<span class="normal"><a href="#__codelineno-19-4">4</a></span>
<span class="normal"><a href="#__codelineno-19-5">5</a></span>
<span class="normal"><a href="#__codelineno-19-6">6</a></span>
<span class="normal"><a href="#__codelineno-19-7">7</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-19-1" name="__codelineno-19-1"></a><span class="n">ignore</span> <span class="o">=</span> <span class="n">target</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span>  <span class="c1"># target: (batch,seqlen) -&gt; (batch*seqlen,), thus, ignore: (batch)</span>
<a id="__codelineno-19-2" name="__codelineno-19-2"></a><span class="n">total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">-</span> <span class="n">ignore</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<a id="__codelineno-19-3" name="__codelineno-19-3"></a><span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">ignore</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># avoid -1 index</span>
<a id="__codelineno-19-4" name="__codelineno-19-4"></a><span class="n">true_dist</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">confidence</span><span class="p">)</span>
<a id="__codelineno-19-5" name="__codelineno-19-5"></a><span class="n">kl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">true_dist</span><span class="p">)</span>  <span class="c1"># KLDivLoss(input,target)</span>
<a id="__codelineno-19-6" name="__codelineno-19-6"></a><span class="n">denom</span> <span class="o">=</span> <span class="n">total</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_length</span> <span class="k">else</span> <span class="n">batch_size</span>
<a id="__codelineno-19-7" name="__codelineno-19-7"></a><span class="k">return</span> <span class="n">kl</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">ignore</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">denom</span>
</code></pre></div></td></tr></table></div>
<h4 id="ctc-loss_1">CTC Loss</h4>
<p>torch.nn.CTCLoss接口支持直接指定batch内各个预测序列的长度<code>hlens</code>和目标序列的长度<code>ys_lens</code>。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-20-1">1</a></span>
<span class="normal"><a href="#__codelineno-20-2">2</a></span>
<span class="normal"><a href="#__codelineno-20-3">3</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-20-1" name="__codelineno-20-1"></a><span class="bp">self</span><span class="o">.</span><span class="n">ctc_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CTCLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction_type</span><span class="p">)</span>
<a id="__codelineno-20-2" name="__codelineno-20-2"></a><span class="c1"># CTCLoss(Log_probs,Targets,Input_lengths,Target_lengths)</span>
<a id="__codelineno-20-3" name="__codelineno-20-3"></a><span class="n">loss_ctc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctc</span><span class="p">(</span><span class="n">encoder_out</span><span class="p">,</span> <span class="n">encoder_out_lens</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">text_lengths</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p><code>hlens</code>是encoder输出的batch中各序列真实长度，除去padding部分的长度，可以通过encoder_mask得到：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-21-1">1</a></span>
<span class="normal"><a href="#__codelineno-21-2">2</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-21-1" name="__codelineno-21-1"></a><span class="c1"># wenet/transformer/asr_model.py</span>
<a id="__codelineno-21-2" name="__codelineno-21-2"></a><span class="n">encoder_out_lens</span> <span class="o">=</span> <span class="n">encoder_mask</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p><code>ys_lens</code>是batch中各标注序列的真实长度，由DataLoader返回的target_lengths得到：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-22-1">1</a></span>
<span class="normal"><a href="#__codelineno-22-2">2</a></span>
<span class="normal"><a href="#__codelineno-22-3">3</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-22-1" name="__codelineno-22-1"></a><span class="c1">## wenet/utils/executor.py</span>
<a id="__codelineno-22-2" name="__codelineno-22-2"></a><span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
<a id="__codelineno-22-3" name="__codelineno-22-3"></a>    <span class="n">key</span><span class="p">,</span> <span class="n">feats</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">feats_lengths</span><span class="p">,</span> <span class="n">target_lengths</span> <span class="o">=</span> <span class="n">batch</span>
</code></pre></div></td></tr></table></div>
<h3 id="_8">自回归</h3>
<p>Attention Decoder是自回归的，每个token只能看到<strong>自己以及左侧的word</strong>，因此其中的attention实现时，每个位置只能和当前位置以及左侧位置进行操作，为了实现该操作，需要引入一个mask。</p>
<h3 id="chunk-based-model">Chunk-based Model</h3>
<p>因为full attention的每一帧都要依赖右侧所有帧，所以无法应用于流式解码中，因此WeNet采用chunk-based attention，将帧分为等大小的chunk，每个chunk只在chunk内部进行attention操作， 另外，也允许和左侧的一定长度的帧进行attention，但<em>这种固定chunk大小的训练模式，要求解码时必须采用同样大小的帧</em>。WeNet引入了一种<em>dynamic chunk training</em>算法，在训练时可以动态为每个batch生成不同大小的 chunk，这样，<em>在解码时，chunk大小可以任意指定</em>，大的chunk可以获得高识别率，小的chunk可以获得低延时，从而用户仅需训练单一模型，根据具体场景在解码时选择合适的chunk大小，而无需重新训练模型。</p>
<h3 id="mask">编码器中的mask</h3>
<p>具体实现位于wenet/transformer/encoder.py</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-23-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-23-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-23-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-23-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-23-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-23-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-23-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-23-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-23-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-23-10">10</a></span>
<span class="normal"><a href="#__codelineno-23-11">11</a></span>
<span class="normal"><a href="#__codelineno-23-12">12</a></span>
<span class="normal"><a href="#__codelineno-23-13">13</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-23-1" name="__codelineno-23-1"></a><span class="n">masks</span> <span class="o">=</span> <span class="o">~</span><span class="n">make_pad_mask</span><span class="p">(</span><span class="n">xs_lens</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, 1, T)</span>
<a id="__codelineno-23-2" name="__codelineno-23-2"></a><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_cmvn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-23-3" name="__codelineno-23-3"></a>    <span class="n">xs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_cmvn</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<a id="__codelineno-23-4" name="__codelineno-23-4"></a><span class="n">xs</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<a id="__codelineno-23-5" name="__codelineno-23-5"></a><span class="n">mask_pad</span> <span class="o">=</span> <span class="n">masks</span>  <span class="c1"># (B, 1, T/subsample_rate)</span>
<a id="__codelineno-23-6" name="__codelineno-23-6"></a><span class="n">chunk_masks</span> <span class="o">=</span> <span class="n">add_optional_chunk_mask</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">masks</span><span class="p">,</span>
<a id="__codelineno-23-7" name="__codelineno-23-7"></a>                                        <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_chunk</span><span class="p">,</span>
<a id="__codelineno-23-8" name="__codelineno-23-8"></a>                                        <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_left_chunk</span><span class="p">,</span>
<a id="__codelineno-23-9" name="__codelineno-23-9"></a>                                        <span class="n">decoding_chunk_size</span><span class="p">,</span>
<a id="__codelineno-23-10" name="__codelineno-23-10"></a>                                        <span class="bp">self</span><span class="o">.</span><span class="n">static_chunk_size</span><span class="p">,</span>
<a id="__codelineno-23-11" name="__codelineno-23-11"></a>                                        <span class="n">num_decoding_left_chunks</span><span class="p">)</span>
<a id="__codelineno-23-12" name="__codelineno-23-12"></a><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="p">:</span>
<a id="__codelineno-23-13" name="__codelineno-23-13"></a>    <span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">chunk_masks</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<ul>
<li><code>self.embed</code>会对原始语音帧填充掩蔽矩阵（frame padding mask）进行降采样得到解码帧填充掩蔽矩阵（subsample frame padding mask）。</li>
<li>
<p><code>mask_pad</code>会在Conformer中的卷积网络模块中使用。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-24-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-24-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-24-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-24-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-24-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-24-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-24-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-24-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-24-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-24-10">10</a></span>
<span class="normal"><a href="#__codelineno-24-11">11</a></span>
<span class="normal"><a href="#__codelineno-24-12">12</a></span>
<span class="normal"><a href="#__codelineno-24-13">13</a></span>
<span class="normal"><a href="#__codelineno-24-14">14</a></span>
<span class="normal"><a href="#__codelineno-24-15">15</a></span>
<span class="normal"><a href="#__codelineno-24-16">16</a></span>
<span class="normal"><a href="#__codelineno-24-17">17</a></span>
<span class="normal"><a href="#__codelineno-24-18">18</a></span>
<span class="normal"><a href="#__codelineno-24-19">19</a></span>
<span class="normal"><a href="#__codelineno-24-20">20</a></span>
<span class="normal"><a href="#__codelineno-24-21">21</a></span>
<span class="normal"><a href="#__codelineno-24-22">22</a></span>
<span class="normal"><a href="#__codelineno-24-23">23</a></span>
<span class="normal"><a href="#__codelineno-24-24">24</a></span>
<span class="normal"><a href="#__codelineno-24-25">25</a></span>
<span class="normal"><a href="#__codelineno-24-26">26</a></span>
<span class="normal"><a href="#__codelineno-24-27">27</a></span>
<span class="normal"><a href="#__codelineno-24-28">28</a></span>
<span class="normal"><a href="#__codelineno-24-29">29</a></span>
<span class="normal"><a href="#__codelineno-24-30">30</a></span>
<span class="normal"><a href="#__codelineno-24-31">31</a></span>
<span class="normal"><a href="#__codelineno-24-32">32</a></span>
<span class="normal"><a href="#__codelineno-24-33">33</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-24-1" name="__codelineno-24-1"></a><span class="k">if</span> <span class="n">mask_pad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-24-2" name="__codelineno-24-2"></a>    <span class="n">x</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">mask_pad</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
<a id="__codelineno-24-3" name="__codelineno-24-3"></a>
<a id="__codelineno-24-4" name="__codelineno-24-4"></a><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-24-5" name="__codelineno-24-5"></a>    <span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-24-6" name="__codelineno-24-6"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lorder</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
<a id="__codelineno-24-7" name="__codelineno-24-7"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-24-8" name="__codelineno-24-8"></a>        <span class="k">assert</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<a id="__codelineno-24-9" name="__codelineno-24-9"></a>        <span class="k">assert</span> <span class="n">cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-24-10" name="__codelineno-24-10"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cache</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-24-11" name="__codelineno-24-11"></a>    <span class="k">assert</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span><span class="p">)</span>
<a id="__codelineno-24-12" name="__codelineno-24-12"></a>    <span class="n">new_cache</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">lorder</span><span class="p">:]</span>
<a id="__codelineno-24-13" name="__codelineno-24-13"></a><span class="k">else</span><span class="p">:</span>
<a id="__codelineno-24-14" name="__codelineno-24-14"></a>    <span class="c1"># It&#39;s better we just return None if no cache is requried,</span>
<a id="__codelineno-24-15" name="__codelineno-24-15"></a>    <span class="c1"># However, for JIT export, here we just fake one tensor instead of</span>
<a id="__codelineno-24-16" name="__codelineno-24-16"></a>    <span class="c1"># None.</span>
<a id="__codelineno-24-17" name="__codelineno-24-17"></a>    <span class="n">new_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<a id="__codelineno-24-18" name="__codelineno-24-18"></a>
<a id="__codelineno-24-19" name="__codelineno-24-19"></a><span class="c1"># GLU mechanism</span>
<a id="__codelineno-24-20" name="__codelineno-24-20"></a><span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch, 2*channel, dim)</span>
<a id="__codelineno-24-21" name="__codelineno-24-21"></a><span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">glu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, channel, dim)</span>
<a id="__codelineno-24-22" name="__codelineno-24-22"></a>
<a id="__codelineno-24-23" name="__codelineno-24-23"></a><span class="c1"># 1D Depthwise Conv</span>
<a id="__codelineno-24-24" name="__codelineno-24-24"></a><span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-24-25" name="__codelineno-24-25"></a><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_layer_norm</span><span class="p">:</span>
<a id="__codelineno-24-26" name="__codelineno-24-26"></a>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-24-27" name="__codelineno-24-27"></a><span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<a id="__codelineno-24-28" name="__codelineno-24-28"></a><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_layer_norm</span><span class="p">:</span>
<a id="__codelineno-24-29" name="__codelineno-24-29"></a>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-24-30" name="__codelineno-24-30"></a><span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise_conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-24-31" name="__codelineno-24-31"></a><span class="c1"># mask batch padding</span>
<a id="__codelineno-24-32" name="__codelineno-24-32"></a><span class="k">if</span> <span class="n">mask_pad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-24-33" name="__codelineno-24-33"></a>    <span class="n">x</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">mask_pad</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</li>
<li>
<p><code>add_optional_chunk_mask</code>会在解码帧填充掩蔽矩阵（subsample frame padding mask）的基础上增加<code>chunk mask</code>，会在Conformer中的self attention使用。</p>
</li>
</ul>
<h4 id="subsamplingmask">下采样网络（Subsampling）中的mask</h4>
<p>subsampling网络中的卷积运算时本身不使用frame padding mask，但是会对frame padding mask降采样得到subsample frame padding mask，后续在进行encoder相关计算时会使用这个subsample frame padding mask。比如在4倍降采样的网络里，使用了两个stride=2的卷积，对shape=(B, 1, L) 的mask进行了<code>mask[:, :, :-2:2][:, :, :-2:2]</code>的操作，新的mask的shape为(B, 1, L/4)。但是目前WeNet的实现存在小问题，最后几个解码帧（降采样帧）如果其卷积野中有padding的帧，则该降采样帧输入无效帧，不应该参与后续计算，但是按照目前mask的实现仍会被使用。举个例子，训练时，下采样倍率（subsample rate）为4，一个解码帧需要7个原始帧。假设某个样本的倒数4个原始帧都是padding的，<em>最后一个解码帧依赖于这些填充的原始帧，因此不应该使用</em>，其mask计算过程如下：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-25-1">1</a></span>
<span class="normal"><a href="#__codelineno-25-2">2</a></span>
<span class="normal"><a href="#__codelineno-25-3">3</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-25-1" name="__codelineno-25-1"></a>1 1 1 1 1 1 1 0 0 0 0  # 原始语音帧
<a id="__codelineno-25-2" name="__codelineno-25-2"></a>1   1   1   1   0      # mask[:, :, :-2:2]
<a id="__codelineno-25-3" name="__codelineno-25-3"></a>1       1              # mask[:, :, :-2:2][:, :, :-2:2]
</code></pre></div></td></tr></table></div>
<p>注意到，计算获得的subsample frame padding mask，仍然会使用最后一个信息不全的解码帧。</p>
<p>假设某个样本倒数5个原始语音帧均是填充的，其mask计算过程如下：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-26-1">1</a></span>
<span class="normal"><a href="#__codelineno-26-2">2</a></span>
<span class="normal"><a href="#__codelineno-26-3">3</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-26-1" name="__codelineno-26-1"></a>1 1 1 1 1 1 0 0 0 0 0  # 原始语音帧
<a id="__codelineno-26-2" name="__codelineno-26-2"></a>1   1   1   1   0      # mask[:, :, :-2:2]
<a id="__codelineno-26-3" name="__codelineno-26-3"></a>1       1              # mask[:, :, :-2:2][:, :, :-2:2]
</code></pre></div></td></tr></table></div>
<p>此时计算获得的subsample frame padding mask表示最后两个解码帧均会用到，但是倒数第二个、第一个的解码帧均有填充的原始帧，信息不全，<em>因此不应该使用</em>。但是实验表明，这影响不大。</p>
<h4 id="conformermask">Conformer中卷积网络模块的mask</h4>
<p><img alt="" src="../attachments/Pasted%20image%2020220529174814.png" /></p>
<p>考虑上图中多层卷积的情况，假设kernel大小为3。 由于batch paddings的存在，在训练时，绿色单元依赖于红色单元，而<em>红色单元不是0值</em>。 然而在解码时，因为一个batch只有一个样本，不需要padding，也就没有batch padding存在，绿色单元依赖的红色单元位置的值<em>总是由</em>conv paddings产生的，这个值<em>总是</em>0值，所以如果不进行任何处理，存在训练和解码不一致的问题。因此，代码里利用subsample frame padding，将每一层的<em>batch padding部分的值变为0</em>。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-27-1">1</a></span>
<span class="normal"><a href="#__codelineno-27-2">2</a></span>
<span class="normal"><a href="#__codelineno-27-3">3</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-27-1" name="__codelineno-27-1"></a><span class="c1"># wenet/transformer/convolution.py</span>
<a id="__codelineno-27-2" name="__codelineno-27-2"></a><span class="k">if</span> <span class="n">mask_pad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-27-3" name="__codelineno-27-3"></a>    <span class="n">x</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">mask_pad</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>如果使用因果卷积，由于其结构特点，每个点均不依赖右侧的点，就不需要这个mask。</p>
<h4 id="multiheadattentionmask">MultiHeadAttention的mask</h4>
<ul>
<li>用于self-attention时，每个样本的mask是一个长和高一样大小的方阵。</li>
<li>用于cross-attention时，mask纵轴从上到下为文本序列,横轴从左到右为帧序列，[batch_size, L, T]。</li>
</ul>
<p>计算attention的权重时使用mask的方式：先将不需要计算的位置的score设为负无穷而不是0，然后计算softmax得到位置权重，此时不需要计算的位置是极小值，然后再把不需要参与attention计算位置的权重系数设为0.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-28-1">1</a></span>
<span class="normal"><a href="#__codelineno-28-2">2</a></span>
<span class="normal"><a href="#__codelineno-28-3">3</a></span>
<span class="normal"><a href="#__codelineno-28-4">4</a></span>
<span class="normal"><a href="#__codelineno-28-5">5</a></span>
<span class="normal"><a href="#__codelineno-28-6">6</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-28-1" name="__codelineno-28-1"></a><span class="c1"># wenet/transformer/attention.py</span>
<a id="__codelineno-28-2" name="__codelineno-28-2"></a><span class="k">def</span> <span class="nf">forward_attention</span> <span class="p">():</span>
<a id="__codelineno-28-3" name="__codelineno-28-3"></a>    <span class="o">...</span>
<a id="__codelineno-28-4" name="__codelineno-28-4"></a>    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (batch, 1, *, time2)</span>
<a id="__codelineno-28-5" name="__codelineno-28-5"></a>    <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span>
<a id="__codelineno-28-6" name="__codelineno-28-6"></a>    <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># (batch, head, time1, time2)</span>
</code></pre></div></td></tr></table></div>
<p><code>mask.unsqueeze(1)</code>是为了增加一个head维度。此时:</p>
<ul>
<li>当用于decoder cross-attention时， mask的shape为(batch, 1, 1, Tmax)， scores的shape为(batch, head, Lmax, Tmax)，第1、第2维会进行broadcast</li>
<li>当用于decoder self-attention时， mask的shape为(batch, 1, Lmax, Lmax)，scores的shape为(batch, head, Lmax, Lmax)，第1维会进行broadcast</li>
<li>当用于encoder self-attention时， mask的shape为(batch, 1, Tmax, Tmax)，scores的shape为(batch, head, Tmax, Tmax)，第1维会进行broadcast</li>
</ul>
<p>第1维会进行broadcast就是说无论哪个head，对于填充位置都要进行mask。</p>
<h4 id="chunk-based-attention">Chunk-based Attention</h4>
<p>为了实现流式解码，编码器中使用了基于chunk的attention，并<em>允许各个batch使用不同的chunk大小</em>。</p>
<p>基于chunk的attention，本质上是<strong>限制attention的作用范围，可以通过attention mask来实现</strong>。</p>
<p><img alt="" src="../attachments/Pasted%20image%2020220529174758.png" /></p>
<ul>
<li>
<p><code>subsequent_chunk_mask</code>用于创建一个固定大小chunk的mask。</p>
<p><code>python
  def subsequent_chunk_mask(
    size: int,
    chunk_size: int,
    num_left_chunks: int = -1,
    device: torch.device = torch.device("cpu"),
) -&gt; torch.Tensor:</code></p>
</li>
<li>
<p><code>add_optional_chunk_mask</code>则用于创建动态大小的chunk的mask。</p>
<p><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-29-1">1</a></span>
<span class="normal"><a href="#__codelineno-29-2">2</a></span>
<span class="normal"><a href="#__codelineno-29-3">3</a></span>
<span class="normal"><a href="#__codelineno-29-4">4</a></span>
<span class="normal"><a href="#__codelineno-29-5">5</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-29-1" name="__codelineno-29-1"></a><span class="k">def</span> <span class="nf">add_optional_chunk_mask</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-29-2" name="__codelineno-29-2"></a>                            <span class="n">use_dynamic_chunk</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-29-3" name="__codelineno-29-3"></a>                            <span class="n">use_dynamic_left_chunk</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-29-4" name="__codelineno-29-4"></a>                            <span class="n">decoding_chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">static_chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<a id="__codelineno-29-5" name="__codelineno-29-5"></a>                            <span class="n">num_decoding_left_chunks</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
</code></pre></div></td></tr></table></div>
如上图所示，其中，
- <code>use_dynamic_chunk=True</code>，各的batch使用随机的chunk mask，
    - 如果<code>use_dynamic_left_chunk=True</code>， 各的batch均采用随机的的left chunk长度的信息。
    - 如果<code>use_dynamic_left_chunk=False</code>， 各的batch均采用开头到当前chunk的信息。
- <code>use_dynamic_chunk=False</code>，
    - <code>static_chunk_size &lt;= 0</code>，采用full-attention。
    - <code>static_chunk_size &gt; 0</code>，采用固定的chunk mask。</p>
</li>
</ul>
<h3 id="mask_1">解码器中的mask</h3>
<p>解码器中涉及到两种Attention：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-30-1">1</a></span>
<span class="normal"><a href="#__codelineno-30-2">2</a></span>
<span class="normal"><a href="#__codelineno-30-3">3</a></span>
<span class="normal"><a href="#__codelineno-30-4">4</a></span>
<span class="normal"><a href="#__codelineno-30-5">5</a></span>
<span class="normal"><a href="#__codelineno-30-6">6</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-30-1" name="__codelineno-30-1"></a><span class="c1"># wenet/transformer/decoder_layer.py</span>
<a id="__codelineno-30-2" name="__codelineno-30-2"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
<a id="__codelineno-30-3" name="__codelineno-30-3"></a>    <span class="o">...</span>
<a id="__codelineno-30-4" name="__codelineno-30-4"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">tgt_q</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_q_mask</span><span class="p">)</span>  <span class="c1"># self-attention</span>
<a id="__codelineno-30-5" name="__codelineno-30-5"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">)</span>  <span class="c1"># cross-attention</span>
<a id="__codelineno-30-6" name="__codelineno-30-6"></a>    <span class="o">...</span>
</code></pre></div></td></tr></table></div>
<h4 id="self-attention">self-attention</h4>
<p>self attention中要考虑自回归和label batch padding带来的影响。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-31-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-31-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-31-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-31-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-31-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-31-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-31-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-31-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-31-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-31-10">10</a></span>
<span class="normal"><a href="#__codelineno-31-11">11</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-31-1" name="__codelineno-31-1"></a><span class="c1"># wenet/transformer/decoder.py</span>
<a id="__codelineno-31-2" name="__codelineno-31-2"></a><span class="n">tgt</span> <span class="o">=</span> <span class="n">ys_in_pad</span>
<a id="__codelineno-31-3" name="__codelineno-31-3"></a><span class="n">maxlen</span> <span class="o">=</span> <span class="n">tgt</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-31-4" name="__codelineno-31-4"></a><span class="c1"># tgt_mask: (B, 1, L)</span>
<a id="__codelineno-31-5" name="__codelineno-31-5"></a><span class="n">tgt_mask</span> <span class="o">=</span> <span class="o">~</span><span class="n">make_pad_mask</span><span class="p">(</span><span class="n">ys_in_lens</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-31-6" name="__codelineno-31-6"></a><span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">tgt_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tgt</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<a id="__codelineno-31-7" name="__codelineno-31-7"></a><span class="c1"># m: (1, L, L)</span>
<a id="__codelineno-31-8" name="__codelineno-31-8"></a><span class="n">m</span> <span class="o">=</span> <span class="n">subsequent_mask</span><span class="p">(</span><span class="n">tgt_mask</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
<a id="__codelineno-31-9" name="__codelineno-31-9"></a>                    <span class="n">device</span><span class="o">=</span><span class="n">tgt_mask</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<a id="__codelineno-31-10" name="__codelineno-31-10"></a><span class="c1"># tgt_mask: (B, L, L)</span>
<a id="__codelineno-31-11" name="__codelineno-31-11"></a><span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">tgt_mask</span> <span class="o">&amp;</span> <span class="n">m</span>
</code></pre></div></td></tr></table></div>
<p>上述代码中，</p>
<ul>
<li><code>~make_pad_mask()</code>函数产生的<code>tgt_mask</code>是标签填充掩蔽矩阵（label padding mask），每个token不允许对padding部分进行attention操作。</li>
<li><code>subsequent_mask()</code>函数产生的<code>m</code>是解码器自回归的mask，每个token只对自己以及左侧的token进行attention操作。</li>
<li>最终的<code>tgt_mask = tgt_mask &amp; m</code>表示在自回归mask和label padding mask同时作用，也就是每个token仅对除了padding部分之外的，自己及其左侧token进行attention。</li>
</ul>
<p><img alt="" src="../attachments/Pasted%20image%2020220529230341.png" /></p>
<p>不过，由于decoder本身是自回归的，自回归掩码保证了对于非padding的位置，均不会去计算自己右侧的位置；并且对于padding位置，在loss中会处理，不参与最后的loss计算，因此，其实并不需要label padding mask，也就是上述代码中最终的tgt_mask。</p>
<h4 id="cross-attention">cross-attention</h4>
<p>编解码器输出进行cross-attention，由于编码器的一些输出是padding，因此需要利用解码帧填充掩蔽矩阵（subsample frame padding mask）指示编码器输出的填充数据位置。</p>
<p><img alt="" src="../attachments/Pasted%20image%2020220529230354.png" /></p>
<h4 id="_9">整体结构</h4>
<p>解码器的每一层均需要计算如上两个attention，解码器内部的self attention需要掩蔽padding和右侧数据，编码器输出需要掩蔽解码帧padding数据，两者利用mask抛除填充的无效数据之后，进行cross-attention。</p>
<p><img alt="" src="../attachments/Pasted%20image%2020220529230403.png" /></p>
<h4 id="_10">其它</h4>
<p>在进行解码时，还用到了<code>mask_finished_scores()</code>和<code>mask_finished_preds()</code>。</p>
<h2 id="wenetcache">WeNet中的cache</h2>
<p>标准的forward是整个序列进行计算，但是在流式推断时，需要chunk级别的forward，因此需要引入cache的概念，即<strong>当前chunk在进行前向计算时，需要拿到上次前向的一些结果作为输入</strong>。对于流式推断，输入是一个一个chunk的到来，*<em>对第i个chunk，当计算第k层网络的输出时，由于网络结构存在对左侧上下文的依赖，需要依赖第k-1层网络里，在第i个chunk之前的一些chunks的输出。 *</em>如果对于当前到来chunk，将其和依赖的chunk序列（比如10层self-attention层，每层依赖左侧4个chunk，则累积起来需要依赖左侧40个chunk）拼起来作为网络输入进行前向，其计算量会比较大。 对于那些已经计算过的chunk，可以将那些在计算下一个chunk的输出时，把需要的中间量保存下来，从而减少重复计算，这种方式就叫cache。另外，WeNet的网络结构在设计时，由于因果卷积和self-attention的左侧上下文都使用有限长度，因此无论序列多长，每次cache的大小是不变的（不增长）。</p>
<p>WeNet<strong>仅仅编码器部分涉及chunk计算时的cache</strong>。 对于CTC解码器，由于是线性层，因此不需要cache；而对于AED的解码器，是在计算完整个编码器输出之后，才进行rescoring，因此不涉及chunk。</p>
<p><img alt="" src="../attachments/Pasted%20image%2020220529232504.png" /></p>
<h3 id="runtime">Runtime流式解码</h3>
<p><code>ASRModel</code>中的<code>forward_encoder_chunk()</code>通过JIT导出，以用于C++ runtime，内部使用<code>BaseEncoder</code>中的<code>forward_chunk()</code>：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-32-1">1</a></span>
<span class="normal"><a href="#__codelineno-32-2">2</a></span>
<span class="normal"><a href="#__codelineno-32-3">3</a></span>
<span class="normal"><a href="#__codelineno-32-4">4</a></span>
<span class="normal"><a href="#__codelineno-32-5">5</a></span>
<span class="normal"><a href="#__codelineno-32-6">6</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-32-1" name="__codelineno-32-1"></a><span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
<a id="__codelineno-32-2" name="__codelineno-32-2"></a><span class="k">def</span> <span class="nf">forward_encoder_chunk</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
<a id="__codelineno-32-3" name="__codelineno-32-3"></a>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward_chunk</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">required_cache_size</span><span class="p">,</span>
<a id="__codelineno-32-4" name="__codelineno-32-4"></a>                                        <span class="n">subsampling_cache</span><span class="p">,</span>
<a id="__codelineno-32-5" name="__codelineno-32-5"></a>                                        <span class="n">elayers_output_cache</span><span class="p">,</span>
<a id="__codelineno-32-6" name="__codelineno-32-6"></a>                                        <span class="n">conformer_cnn_cache</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<h3 id="python">Python流式解码</h3>
<p><code>ASRModel</code>中进行Python解码的<code>recognize()</code>函数如下：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-33-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-33-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-33-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-33-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-33-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-33-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-33-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-33-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-33-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-33-10">10</a></span>
<span class="normal"><a href="#__codelineno-33-11">11</a></span>
<span class="normal"><a href="#__codelineno-33-12">12</a></span>
<span class="normal"><a href="#__codelineno-33-13">13</a></span>
<span class="normal"><a href="#__codelineno-33-14">14</a></span>
<span class="normal"><a href="#__codelineno-33-15">15</a></span>
<span class="normal"><a href="#__codelineno-33-16">16</a></span>
<span class="normal"><a href="#__codelineno-33-17">17</a></span>
<span class="normal"><a href="#__codelineno-33-18">18</a></span>
<span class="normal"><a href="#__codelineno-33-19">19</a></span>
<span class="normal"><a href="#__codelineno-33-20">20</a></span>
<span class="normal"><a href="#__codelineno-33-21">21</a></span>
<span class="normal"><a href="#__codelineno-33-22">22</a></span>
<span class="normal"><a href="#__codelineno-33-23">23</a></span>
<span class="normal"><a href="#__codelineno-33-24">24</a></span>
<span class="normal"><a href="#__codelineno-33-25">25</a></span>
<span class="normal"><a href="#__codelineno-33-26">26</a></span>
<span class="normal"><a href="#__codelineno-33-27">27</a></span>
<span class="normal"><a href="#__codelineno-33-28">28</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-33-1" name="__codelineno-33-1"></a><span class="c1"># wenet/transformer/asr_model.py</span>
<a id="__codelineno-33-2" name="__codelineno-33-2"></a><span class="k">def</span> <span class="nf">recognize</span><span class="p">(</span>
<a id="__codelineno-33-3" name="__codelineno-33-3"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-33-4" name="__codelineno-33-4"></a>    <span class="n">speech</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-33-5" name="__codelineno-33-5"></a>    <span class="n">speech_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-33-6" name="__codelineno-33-6"></a>    <span class="n">beam_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
<a id="__codelineno-33-7" name="__codelineno-33-7"></a>    <span class="n">decoding_chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<a id="__codelineno-33-8" name="__codelineno-33-8"></a>    <span class="n">num_decoding_left_chunks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<a id="__codelineno-33-9" name="__codelineno-33-9"></a>    <span class="n">simulate_streaming</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-33-10" name="__codelineno-33-10"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-33-11" name="__codelineno-33-11"></a>    <span class="sd">&quot;&quot;&quot; Apply beam search on attention decoder</span>
<a id="__codelineno-33-12" name="__codelineno-33-12"></a>
<a id="__codelineno-33-13" name="__codelineno-33-13"></a><span class="sd">    Args:</span>
<a id="__codelineno-33-14" name="__codelineno-33-14"></a><span class="sd">        speech (torch.Tensor): (batch, max_len, feat_dim)</span>
<a id="__codelineno-33-15" name="__codelineno-33-15"></a><span class="sd">        speech_length (torch.Tensor): (batch, )</span>
<a id="__codelineno-33-16" name="__codelineno-33-16"></a><span class="sd">        beam_size (int): beam size for beam search</span>
<a id="__codelineno-33-17" name="__codelineno-33-17"></a><span class="sd">        decoding_chunk_size (int): decoding chunk for dynamic chunk</span>
<a id="__codelineno-33-18" name="__codelineno-33-18"></a><span class="sd">            trained model.</span>
<a id="__codelineno-33-19" name="__codelineno-33-19"></a><span class="sd">            &lt;0: for decoding, use full chunk.</span>
<a id="__codelineno-33-20" name="__codelineno-33-20"></a><span class="sd">            &gt;0: for decoding, use fixed chunk size as set.</span>
<a id="__codelineno-33-21" name="__codelineno-33-21"></a><span class="sd">            0: used for training, it&#39;s prohibited here</span>
<a id="__codelineno-33-22" name="__codelineno-33-22"></a><span class="sd">        simulate_streaming (bool): whether do encoder forward in a</span>
<a id="__codelineno-33-23" name="__codelineno-33-23"></a><span class="sd">            streaming fashion</span>
<a id="__codelineno-33-24" name="__codelineno-33-24"></a>
<a id="__codelineno-33-25" name="__codelineno-33-25"></a><span class="sd">    Returns:</span>
<a id="__codelineno-33-26" name="__codelineno-33-26"></a><span class="sd">        torch.Tensor: decoding result, (batch, max_result_len)</span>
<a id="__codelineno-33-27" name="__codelineno-33-27"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-33-28" name="__codelineno-33-28"></a>    <span class="o">...</span>
</code></pre></div></td></tr></table></div>
<p>如果设置<code>simulate_streaming</code>为True，就会模拟runtime流式解码的过程，将数据分成chunk，依次进行前向计算，该方法的结果，和送入整个序列通过mask进行流式模拟的结果应该是一致的：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-34-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-34-1" name="__codelineno-34-1"></a>recognize() -&gt; _forward_encoder() -&gt; BaseEncoder.forward_chunk_by_chunk() -&gt; BaseEncoder.forward_chunk()
</code></pre></div></td></tr></table></div>
<p>可以看到，最终调用的还是<code>BaseEncoder</code>中的<code>forward_chunk()</code>函数。</p>
<h3 id="baseencoderforward_chunk">BaseEncoder.forward_chunk()分析</h3>
<p>在chunk流式计算时，需要特别注意三个方面的缓存：</p>
<ul>
<li>下采样（subsampling）中的卷积。但是subsampling内部不进行cache，一是实现比较复杂，二是subsampling的计算量不大。</li>
<li>Transformer/Conformer编码器中每一层的输出。</li>
<li>Conformer中的卷积。</li>
</ul>
<p><code>forward_chunk()</code>是对<strong>单个chunk进行前向计算的核心函数</strong>：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-35-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-35-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-35-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-35-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-35-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-35-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-35-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-35-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-35-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-35-10">10</a></span>
<span class="normal"><a href="#__codelineno-35-11">11</a></span>
<span class="normal"><a href="#__codelineno-35-12">12</a></span>
<span class="normal"><a href="#__codelineno-35-13">13</a></span>
<span class="normal"><a href="#__codelineno-35-14">14</a></span>
<span class="normal"><a href="#__codelineno-35-15">15</a></span>
<span class="normal"><a href="#__codelineno-35-16">16</a></span>
<span class="normal"><a href="#__codelineno-35-17">17</a></span>
<span class="normal"><a href="#__codelineno-35-18">18</a></span>
<span class="normal"><a href="#__codelineno-35-19">19</a></span>
<span class="normal"><a href="#__codelineno-35-20">20</a></span>
<span class="normal"><a href="#__codelineno-35-21">21</a></span>
<span class="normal"><a href="#__codelineno-35-22">22</a></span>
<span class="normal"><a href="#__codelineno-35-23">23</a></span>
<span class="normal"><a href="#__codelineno-35-24">24</a></span>
<span class="normal"><a href="#__codelineno-35-25">25</a></span>
<span class="normal"><a href="#__codelineno-35-26">26</a></span>
<span class="normal"><a href="#__codelineno-35-27">27</a></span>
<span class="normal"><a href="#__codelineno-35-28">28</a></span>
<span class="normal"><a href="#__codelineno-35-29">29</a></span>
<span class="normal"><a href="#__codelineno-35-30">30</a></span>
<span class="normal"><a href="#__codelineno-35-31">31</a></span>
<span class="normal"><a href="#__codelineno-35-32">32</a></span>
<span class="normal"><a href="#__codelineno-35-33">33</a></span>
<span class="normal"><a href="#__codelineno-35-34">34</a></span>
<span class="normal"><a href="#__codelineno-35-35">35</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-35-1" name="__codelineno-35-1"></a><span class="c1"># wenet/transformer/encoder.py</span>
<a id="__codelineno-35-2" name="__codelineno-35-2"></a><span class="k">def</span> <span class="nf">forward_chunk</span><span class="p">(</span>
<a id="__codelineno-35-3" name="__codelineno-35-3"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-35-4" name="__codelineno-35-4"></a>    <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-35-5" name="__codelineno-35-5"></a>    <span class="n">offset</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<a id="__codelineno-35-6" name="__codelineno-35-6"></a>    <span class="n">required_cache_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<a id="__codelineno-35-7" name="__codelineno-35-7"></a>    <span class="n">subsampling_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-35-8" name="__codelineno-35-8"></a>    <span class="n">elayers_output_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-35-9" name="__codelineno-35-9"></a>    <span class="n">conformer_cnn_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-35-10" name="__codelineno-35-10"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-35-11" name="__codelineno-35-11"></a>            <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<a id="__codelineno-35-12" name="__codelineno-35-12"></a>    <span class="sd">&quot;&quot;&quot; Forward just one chunk</span>
<a id="__codelineno-35-13" name="__codelineno-35-13"></a>
<a id="__codelineno-35-14" name="__codelineno-35-14"></a><span class="sd">    Args:</span>
<a id="__codelineno-35-15" name="__codelineno-35-15"></a><span class="sd">        xs (torch.Tensor): chunk input</span>
<a id="__codelineno-35-16" name="__codelineno-35-16"></a><span class="sd">        offset (int): current offset in encoder output time stamp</span>
<a id="__codelineno-35-17" name="__codelineno-35-17"></a><span class="sd">        required_cache_size (int): cache size required for next chunk</span>
<a id="__codelineno-35-18" name="__codelineno-35-18"></a><span class="sd">            compuation</span>
<a id="__codelineno-35-19" name="__codelineno-35-19"></a><span class="sd">            &gt;=0: actual cache size</span>
<a id="__codelineno-35-20" name="__codelineno-35-20"></a><span class="sd">            &lt;0: means all history cache is required</span>
<a id="__codelineno-35-21" name="__codelineno-35-21"></a><span class="sd">        subsampling_cache (Optional[torch.Tensor]): subsampling cache</span>
<a id="__codelineno-35-22" name="__codelineno-35-22"></a><span class="sd">        elayers_output_cache (Optional[List[torch.Tensor]]):</span>
<a id="__codelineno-35-23" name="__codelineno-35-23"></a><span class="sd">            transformer/conformer encoder layers output cache</span>
<a id="__codelineno-35-24" name="__codelineno-35-24"></a><span class="sd">        conformer_cnn_cache (Optional[List[torch.Tensor]]): conformer</span>
<a id="__codelineno-35-25" name="__codelineno-35-25"></a><span class="sd">            cnn cache</span>
<a id="__codelineno-35-26" name="__codelineno-35-26"></a>
<a id="__codelineno-35-27" name="__codelineno-35-27"></a><span class="sd">    Returns:</span>
<a id="__codelineno-35-28" name="__codelineno-35-28"></a><span class="sd">        torch.Tensor: output of current input xs</span>
<a id="__codelineno-35-29" name="__codelineno-35-29"></a><span class="sd">        torch.Tensor: subsampling cache required for next chunk computation</span>
<a id="__codelineno-35-30" name="__codelineno-35-30"></a><span class="sd">        List[torch.Tensor]: encoder layers output cache required for next</span>
<a id="__codelineno-35-31" name="__codelineno-35-31"></a><span class="sd">            chunk computation</span>
<a id="__codelineno-35-32" name="__codelineno-35-32"></a><span class="sd">        List[torch.Tensor]: conformer cnn cache</span>
<a id="__codelineno-35-33" name="__codelineno-35-33"></a>
<a id="__codelineno-35-34" name="__codelineno-35-34"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-35-35" name="__codelineno-35-35"></a>    <span class="o">...</span>
</code></pre></div></td></tr></table></div>
<p>上述代码中，<code>xs</code>是当前chunk输入。由于单个chunk的前向计算，需要之前chunk计算得到的信息，因此这里需要传入相关的三个cache信息：</p>
<ul>
<li><code>subsampling_cache</code>: [torch.Tensor]，subsampling输出的cache，也就是第一个Conformer block的输入。</li>
<li><code>elayers_output_cache</code>: List[torch.Tensor]，第1个到最后一个Conformer block的输出的cache，也就是第2个Conformer block的输入和CTC层的输入。</li>
<li><code>conformer_cnn_cache</code>: List[torch.Tensor]，Conformer block里的卷积层左侧依赖的输入cache。</li>
</ul>
<h4 id="cache">cache的大小</h4>
<ul>
<li>
<p><code>subsampling_cache</code>和<code>elayers_output_cache</code>的大小是由self-attention对左侧的依赖长度<code>required_cache_size</code>决定。<code>decoding_chunk_size</code>表示解码帧级别的chunk大小，<code>num_decoding_left_chunks</code>是self-attention依赖的左侧chunk数目，<code>required_cache_size</code>表示self-attention依赖的左侧解码帧总数，因此满足：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-36-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-36-1" name="__codelineno-36-1"></a>required_cache_size = decoding_chunk_size * num_decoding_left_chunks
</code></pre></div></td></tr></table></div>
</li>
<li>
<p><code>conformer_cnn_cache</code>的大小和<code>required_cache_size</code>无关，只由因果卷积的左侧上下文<code>lorder</code>参数决定，<code>lorder</code>就是实现因果卷积的左侧填充数：</p>
</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-37-1">1</a></span>
<span class="normal"><a href="#__codelineno-37-2">2</a></span>
<span class="normal"><a href="#__codelineno-37-3">3</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-37-1" name="__codelineno-37-1"></a><span class="k">if</span> <span class="n">causal</span><span class="p">:</span>
<a id="__codelineno-37-2" name="__codelineno-37-2"></a>    <span class="n">padding</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-37-3" name="__codelineno-37-3"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span>
</code></pre></div></td></tr></table></div>
<p>函数返回了四个值，包括当前chunk输入对应的输出，以及更新后的三个cache。</p>
<p><img alt="" src="../attachments/Pasted%20image%2020220529175021.png" /></p>
<h4 id="offset">offset</h4>
<p>当按chunk进行输入时，不能直接得到chunk在序列中的位置，因此需要传入offset给出该chunk在整个序列里的偏移，用于计算positional encoding：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-38-1">1</a></span>
<span class="normal"><a href="#__codelineno-38-2">2</a></span>
<span class="normal"><a href="#__codelineno-38-3">3</a></span>
<span class="normal"><a href="#__codelineno-38-4">4</a></span>
<span class="normal"><a href="#__codelineno-38-5">5</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-38-1" name="__codelineno-38-1"></a><span class="c1"># wenet/transformer/encoder.py</span>
<a id="__codelineno-38-2" name="__codelineno-38-2"></a><span class="k">def</span> <span class="nf">forward_chunk</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
<a id="__codelineno-38-3" name="__codelineno-38-3"></a>    <span class="o">...</span>
<a id="__codelineno-38-4" name="__codelineno-38-4"></a>    <span class="n">xs</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">tmp_masks</span><span class="p">,</span> <span class="n">offset</span><span class="p">)</span>
<a id="__codelineno-38-5" name="__codelineno-38-5"></a>    <span class="o">...</span>
</code></pre></div></td></tr></table></div>
<h4 id="subsampling_cache">subsampling_cache</h4>
<p>subsampling输出的cache，即第一个Conformer block的输入。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-39-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-39-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-39-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-39-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-39-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-39-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-39-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-39-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-39-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-39-10">10</a></span>
<span class="normal"><a href="#__codelineno-39-11">11</a></span>
<span class="normal"><a href="#__codelineno-39-12">12</a></span>
<span class="normal"><a href="#__codelineno-39-13">13</a></span>
<span class="normal"><a href="#__codelineno-39-14">14</a></span>
<span class="normal"><a href="#__codelineno-39-15">15</a></span>
<span class="normal"><a href="#__codelineno-39-16">16</a></span>
<span class="normal"><a href="#__codelineno-39-17">17</a></span>
<span class="normal"><a href="#__codelineno-39-18">18</a></span>
<span class="normal"><a href="#__codelineno-39-19">19</a></span>
<span class="normal"><a href="#__codelineno-39-20">20</a></span>
<span class="normal"><a href="#__codelineno-39-21">21</a></span>
<span class="normal"><a href="#__codelineno-39-22">22</a></span>
<span class="normal"><a href="#__codelineno-39-23">23</a></span>
<span class="normal"><a href="#__codelineno-39-24">24</a></span>
<span class="normal"><a href="#__codelineno-39-25">25</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-39-1" name="__codelineno-39-1"></a><span class="k">def</span> <span class="nf">forward_chunk</span><span class="p">(</span>
<a id="__codelineno-39-2" name="__codelineno-39-2"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-39-3" name="__codelineno-39-3"></a>    <span class="n">xs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-39-4" name="__codelineno-39-4"></a>    <span class="n">offset</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<a id="__codelineno-39-5" name="__codelineno-39-5"></a>    <span class="n">required_cache_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<a id="__codelineno-39-6" name="__codelineno-39-6"></a>    <span class="n">subsampling_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-39-7" name="__codelineno-39-7"></a>    <span class="n">elayers_output_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-39-8" name="__codelineno-39-8"></a>    <span class="n">conformer_cnn_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-39-9" name="__codelineno-39-9"></a><span class="p">):</span>
<a id="__codelineno-39-10" name="__codelineno-39-10"></a>    <span class="o">...</span>
<a id="__codelineno-39-11" name="__codelineno-39-11"></a>    <span class="k">if</span> <span class="n">subsampling_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-39-12" name="__codelineno-39-12"></a>        <span class="n">cache_size</span> <span class="o">=</span> <span class="n">subsampling_cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-39-13" name="__codelineno-39-13"></a>        <span class="c1"># xs是第一个conformer block的输入</span>
<a id="__codelineno-39-14" name="__codelineno-39-14"></a>        <span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">subsampling_cache</span><span class="p">,</span> <span class="n">xs</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-39-15" name="__codelineno-39-15"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-39-16" name="__codelineno-39-16"></a>        <span class="n">cache_size</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-39-17" name="__codelineno-39-17"></a>    <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="o">.</span><span class="n">position_encoding</span><span class="p">(</span><span class="n">offset</span> <span class="o">-</span> <span class="n">cache_size</span><span class="p">,</span> <span class="n">xs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<a id="__codelineno-39-18" name="__codelineno-39-18"></a>    <span class="k">if</span> <span class="n">required_cache_size</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-39-19" name="__codelineno-39-19"></a>        <span class="n">next_cache_start</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-39-20" name="__codelineno-39-20"></a>    <span class="k">elif</span> <span class="n">required_cache_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-39-21" name="__codelineno-39-21"></a>        <span class="n">next_cache_start</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-39-22" name="__codelineno-39-22"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-39-23" name="__codelineno-39-23"></a>        <span class="n">next_cache_start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">xs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">required_cache_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<a id="__codelineno-39-24" name="__codelineno-39-24"></a>    <span class="c1"># 更新subsampling_cache</span>
<a id="__codelineno-39-25" name="__codelineno-39-25"></a>    <span class="n">r_subsampling_cache</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[:,</span> <span class="n">next_cache_start</span><span class="p">:,</span> <span class="p">:]</span>
</code></pre></div></td></tr></table></div>
<h4 id="elayers_output_cache">elayers_output_cache</h4>
<p>第1个到最后一个Conformer block的输出的cache，也就是第2个Conformer block的输入和CTC层的输入。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-40-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-40-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-40-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-40-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-40-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-40-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-40-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-40-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-40-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-40-10">10</a></span>
<span class="normal"><a href="#__codelineno-40-11">11</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-40-1" name="__codelineno-40-1"></a><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="p">):</span>
<a id="__codelineno-40-2" name="__codelineno-40-2"></a>    <span class="n">attn_cache</span> <span class="o">=</span> <span class="n">elayers_output_cache</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-40-3" name="__codelineno-40-3"></a>    <span class="n">cnn_cache</span> <span class="o">=</span> <span class="n">conformer_cnn_cache</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-40-4" name="__codelineno-40-4"></a>    <span class="c1"># layer(...)对应ConformerEncoderLayer.forward(...)</span>
<a id="__codelineno-40-5" name="__codelineno-40-5"></a>    <span class="n">xs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">new_cnn_cache</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span>
<a id="__codelineno-40-6" name="__codelineno-40-6"></a>        <span class="n">masks</span><span class="p">,</span>
<a id="__codelineno-40-7" name="__codelineno-40-7"></a>        <span class="n">pos_emb</span><span class="p">,</span>
<a id="__codelineno-40-8" name="__codelineno-40-8"></a>        <span class="n">output_cache</span><span class="o">=</span><span class="n">attn_cache</span><span class="p">,</span>
<a id="__codelineno-40-9" name="__codelineno-40-9"></a>        <span class="n">cnn_cache</span><span class="o">=</span><span class="n">cnn_cache</span><span class="p">)</span>
<a id="__codelineno-40-10" name="__codelineno-40-10"></a>    <span class="c1"># 更新elayers_output_cache</span>
<a id="__codelineno-40-11" name="__codelineno-40-11"></a>    <span class="n">r_elayers_output_cache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xs</span><span class="p">[:,</span> <span class="n">next_cache_start</span><span class="p">:,</span> <span class="p">:])</span>
</code></pre></div></td></tr></table></div>
<p>注意，此处的xs不是当前的chunk，而是当前chunk+cache输入，所以其长度不是chunk_size，而是<code>chunk_size + required_cache_size</code>：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-41-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-41-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-41-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-41-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-41-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-41-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-41-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-41-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-41-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-41-10">10</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-41-1" name="__codelineno-41-1"></a><span class="c1"># wenet/transformer/encoder.py</span>
<a id="__codelineno-41-2" name="__codelineno-41-2"></a><span class="c1"># BaseEncoder.forward_chunk()</span>
<a id="__codelineno-41-3" name="__codelineno-41-3"></a><span class="c1"># 第一个conformer block输入的xs</span>
<a id="__codelineno-41-4" name="__codelineno-41-4"></a><span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">subsampling_cache</span><span class="p">,</span> <span class="n">xs</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-41-5" name="__codelineno-41-5"></a>
<a id="__codelineno-41-6" name="__codelineno-41-6"></a><span class="c1"># wenet/transformer/encoder_layer.py</span>
<a id="__codelineno-41-7" name="__codelineno-41-7"></a><span class="c1"># ConformerEncoderLayer.forward()</span>
<a id="__codelineno-41-8" name="__codelineno-41-8"></a><span class="c1"># 之后的conformer block输入的xs</span>
<a id="__codelineno-41-9" name="__codelineno-41-9"></a><span class="k">if</span> <span class="n">output_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-41-10" name="__codelineno-41-10"></a>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">output_cache</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>此外，上述代码中的<code>layer(...)</code>对应着wenet/transformer/encoder_layer.py中的ConformerEncoderLayer.forward()，具体的计算过程：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-42-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-42-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-42-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-42-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-42-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-42-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-42-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-42-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-42-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-42-10">10</a></span>
<span class="normal"><a href="#__codelineno-42-11">11</a></span>
<span class="normal"><a href="#__codelineno-42-12">12</a></span>
<span class="normal"><a href="#__codelineno-42-13">13</a></span>
<span class="normal"><a href="#__codelineno-42-14">14</a></span>
<span class="normal"><a href="#__codelineno-42-15">15</a></span>
<span class="normal"><a href="#__codelineno-42-16">16</a></span>
<span class="normal"><a href="#__codelineno-42-17">17</a></span>
<span class="normal"><a href="#__codelineno-42-18">18</a></span>
<span class="normal"><a href="#__codelineno-42-19">19</a></span>
<span class="normal"><a href="#__codelineno-42-20">20</a></span>
<span class="normal"><a href="#__codelineno-42-21">21</a></span>
<span class="normal"><a href="#__codelineno-42-22">22</a></span>
<span class="normal"><a href="#__codelineno-42-23">23</a></span>
<span class="normal"><a href="#__codelineno-42-24">24</a></span>
<span class="normal"><a href="#__codelineno-42-25">25</a></span>
<span class="normal"><a href="#__codelineno-42-26">26</a></span>
<span class="normal"><a href="#__codelineno-42-27">27</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-42-1" name="__codelineno-42-1"></a><span class="c1"># 计算feedforwad/res/norm(包含当前chunk和左侧num_decoding_left_chunks个chunk)</span>
<a id="__codelineno-42-2" name="__codelineno-42-2"></a>
<a id="__codelineno-42-3" name="__codelineno-42-3"></a><span class="c1"># 使用cache时，只要计算当前chunk的x_q的self-attentionattention和residual</span>
<a id="__codelineno-42-4" name="__codelineno-42-4"></a>
<a id="__codelineno-42-5" name="__codelineno-42-5"></a><span class="n">chunk</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">output_cache</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-42-6" name="__codelineno-42-6"></a><span class="n">x_q</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="o">-</span><span class="n">chunk</span><span class="p">:,</span> <span class="p">:]</span>
<a id="__codelineno-42-7" name="__codelineno-42-7"></a>
<a id="__codelineno-42-8" name="__codelineno-42-8"></a><span class="c1"># 只选择当前chunk对应的部分做residual计算</span>
<a id="__codelineno-42-9" name="__codelineno-42-9"></a><span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span><span class="p">[:,</span> <span class="o">-</span><span class="n">chunk</span><span class="p">:,</span> <span class="p">:]</span>
<a id="__codelineno-42-10" name="__codelineno-42-10"></a>
<a id="__codelineno-42-11" name="__codelineno-42-11"></a><span class="c1"># 选取当前chunk对应的mask，</span>
<a id="__codelineno-42-12" name="__codelineno-42-12"></a><span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[:,</span> <span class="o">-</span><span class="n">chunk</span><span class="p">:,</span> <span class="p">:]</span>
<a id="__codelineno-42-13" name="__codelineno-42-13"></a>
<a id="__codelineno-42-14" name="__codelineno-42-14"></a><span class="c1"># 使用当前chunk的x_q去和其依赖的x做attention</span>
<a id="__codelineno-42-15" name="__codelineno-42-15"></a><span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x_q</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">))</span>
<a id="__codelineno-42-16" name="__codelineno-42-16"></a>
<a id="__codelineno-42-17" name="__codelineno-42-17"></a><span class="c1"># 仅计算当前chunk的conv</span>
<a id="__codelineno-42-18" name="__codelineno-42-18"></a><span class="n">x</span><span class="p">,</span> <span class="n">new_cnn_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">,</span> <span class="n">cnn_cache</span><span class="p">)</span>
<a id="__codelineno-42-19" name="__codelineno-42-19"></a>
<a id="__codelineno-42-20" name="__codelineno-42-20"></a><span class="c1"># 仅计算当前chunk的feedforwad/res/norm</span>
<a id="__codelineno-42-21" name="__codelineno-42-21"></a><span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-42-22" name="__codelineno-42-22"></a><span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<a id="__codelineno-42-23" name="__codelineno-42-23"></a>
<a id="__codelineno-42-24" name="__codelineno-42-24"></a><span class="c1"># 可以看到通过cache节省了x[:, :-chunk, :]部分的attention/conv以及之后的feedforwad/res/norm计算</span>
<a id="__codelineno-42-25" name="__codelineno-42-25"></a>
<a id="__codelineno-42-26" name="__codelineno-42-26"></a><span class="c1"># chunk的输出和cache拼在一起，作为网络的最终输出。</span>
<a id="__codelineno-42-27" name="__codelineno-42-27"></a><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">output_cache</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>如上述代码所示，通过cache节省了x[:, :-chunk, :]部分的attention/conv以及之后的feedforwad/res/norm计算。</p>
<h4 id="conformer_cnn_cache">conformer_cnn_cache</h4>
<p>Conformer Block中的卷积层左侧依赖的输入cache。conformer_cnn_cache大小为<code>lorder</code>，即因果卷积左侧依赖，<code>lorder</code>就是实现因果卷积的左侧填充数。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-43-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-43-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-43-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-43-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-43-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-43-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-43-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-43-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-43-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-43-10">10</a></span>
<span class="normal"><a href="#__codelineno-43-11">11</a></span>
<span class="normal"><a href="#__codelineno-43-12">12</a></span>
<span class="normal"><a href="#__codelineno-43-13">13</a></span>
<span class="normal"><a href="#__codelineno-43-14">14</a></span>
<span class="normal"><a href="#__codelineno-43-15">15</a></span>
<span class="normal"><a href="#__codelineno-43-16">16</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-43-1" name="__codelineno-43-1"></a><span class="c1"># wenet/transformer/encoder_layer.py</span>
<a id="__codelineno-43-2" name="__codelineno-43-2"></a><span class="c1"># ConformerEncoderLayer.forward()</span>
<a id="__codelineno-43-3" name="__codelineno-43-3"></a><span class="c1"># conformer_cnn_cache通过ConvolutionModule.forward()返回的新cache来更新</span>
<a id="__codelineno-43-4" name="__codelineno-43-4"></a><span class="c1"># self.conv_module(...)参数中的cnn_cache为上一个chunk输出的new_cache</span>
<a id="__codelineno-43-5" name="__codelineno-43-5"></a><span class="n">x</span><span class="p">,</span> <span class="n">new_cnn_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_module</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask_pad</span><span class="p">,</span> <span class="n">cnn_cache</span><span class="p">)</span>
<a id="__codelineno-43-6" name="__codelineno-43-6"></a>
<a id="__codelineno-43-7" name="__codelineno-43-7"></a>
<a id="__codelineno-43-8" name="__codelineno-43-8"></a><span class="c1"># new_cnn_cache的计算过程来自：wenet/transformer/convolution.py</span>
<a id="__codelineno-43-9" name="__codelineno-43-9"></a><span class="c1"># ConvolutionModule.forward()</span>
<a id="__codelineno-43-10" name="__codelineno-43-10"></a><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lorder</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-43-11" name="__codelineno-43-11"></a>    <span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-43-12" name="__codelineno-43-12"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lorder</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
<a id="__codelineno-43-13" name="__codelineno-43-13"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-43-14" name="__codelineno-43-14"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cache</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-43-15" name="__codelineno-43-15"></a>    <span class="c1"># 更新 conformer_cnn_cache</span>
<a id="__codelineno-43-16" name="__codelineno-43-16"></a>    <span class="n">new_cache</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">lorder</span><span class="p">:]</span>
</code></pre></div></td></tr></table></div>
<h2 id="wenet_2">WeNet损失函数</h2>
<p>模型训练时同时使用CTC损失和交叉熵（Cross-Entropy，CE）损失。为了避免过拟合，提高模型模型的泛化能力，在WeNet中使用Label Smoothing代替交叉熵损失。</p>
<p>编码器输出特征进入CTC解码器得到CTC损失，进入Attention解码器获得Label Smoothing损失，并利用<code>ctc_weight</code>进行加权求得最终的损失。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-44-1">1</a></span>
<span class="normal"><a href="#__codelineno-44-2">2</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-44-1" name="__codelineno-44-1"></a><span class="o">//</span> <span class="n">wenet</span><span class="o">/</span><span class="n">wenet</span><span class="o">/</span><span class="n">transformer</span><span class="o">/</span><span class="n">asr_model</span><span class="o">.</span><span class="n">py</span>
<a id="__codelineno-44-2" name="__codelineno-44-2"></a><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctc_weight</span> <span class="o">*</span> <span class="n">loss_ctc</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctc_weight</span><span class="p">)</span> <span class="o">*</span> <span class="n">loss_att</span>
</code></pre></div></td></tr></table></div>
<h2 id="wenet_3">WeNet多机并行训练</h2>
<h3 id="_11">业界现有方案</h3>
<p>目前基于PyTorch的主流分布式训练方案主要有PyTorch原生的DDP（DistributedDataParallel）和horovod两种。</p>
<h3 id="distributeddataparallel">DistributedDataParallel</h3>
<h4 id="pythonglobal-interpreter-lockgil">Python全局解释器锁(Global Interpreter Lock，GIL)</h4>
<p>Python的每个线程获取GIL，然后执行代码直到sleep或者是python虚拟机将其挂起，最后释放GIL，<em>只有拿到GIL的线程才可以运行</em>。Python GIL的存在使得一个python进程只能利用一个CPU核心，不适用于神经网络训练等计算密集型的任务。<em>使用多进程，才能更有效率地利用多核的计算资源</em>。</p>
<h4 id="dpdataparallel">DP（DataParallel）</h4>
<p>DP（DataParallel）是PyTorch早期的数据并行方案，是一种基于Parameter Server的算法，<em>应用于单机多卡的训练</em>。这一方案在存在比较严重的内存和通信流量负载不均衡的问题，其中用于同步各节点信息的主GPU节点的内存一般会比其他GPU节点占用的内存更多。但是真正限制DP的是Parameter Server会使多卡训练时主GPU节点需求的通信带宽远远高于其他GPU节点，导致主节点的通信带宽成为系统的瓶颈。此外其采用了单进程多线程的方式进行训练，性能会受到python GIL的影响。</p>
<h4 id="ddpdp">DDP为什么会比DP要快？</h4>
<p>DDP方案和DP都是数据并行方案，但是DDP采用了Ring-AllReduce数据交换算法，从而提高了通讯效率。该方法每个节点不再全部向主节进行通信，而是只与相邻的节点进行通信，因此每个GPU节点上的流量通信压力相同。此外DDP通过多进程（DDP支持为每个GPU使用一个独立进程）的方式也避免了Python GIL（Global Interpreter Lock）的限制，从而可以进一步提高训练速度。</p>
<h4 id="ddp">DDP线性加速比</h4>
<h3 id="ddp_1">DDP中的概念</h3>
<p>Pytorch中DDP的初始化接口如下：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-45-1">1</a></span>
<span class="normal"><a href="#__codelineno-45-2">2</a></span>
<span class="normal"><a href="#__codelineno-45-3">3</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-45-1" name="__codelineno-45-1"></a>torch.distributed.init_process_group(backend, init_method=None, 
<a id="__codelineno-45-2" name="__codelineno-45-2"></a>    timeout=datetime.timedelta(seconds=1800), world_size=- 1, 
<a id="__codelineno-45-3" name="__codelineno-45-3"></a>    rank=- 1, store=None, group_name=&#39;&#39;, pg_options=None)
</code></pre></div></td></tr></table></div>
<p>其中，</p>
<ul>
<li>backend。用来指定多进程间的通信后端，包括NCCL，Gloo，MPI。其中NCCL是官方最推荐的，因此通常我们直接使用NCCL即可。</li>
<li>init_method。用来表示在启动多进程训练时，各进程的握手方式，主要包括三种方式，即file（共享文件）、tcp（套接字）和env（环境变量），多机训练主要使用file和tcp的方式。通过file的握手方式需要多机之间装有NFS（Network File System），在指定了各进程均可访问的共享文件路径后，各台机器上的不同进程通过该共享文件完成握手，例如 file:///export/nfs/ddp_init。tcp的方式需要给定各机器上不同进程均可访问的网络地址和端口号来完成各进程的握手，例如tcp://127.0.0.1:23456。</li>
<li>group。默认值即可。</li>
<li>world size。表示全局并行的进程数，DDP模式下，<em>最优的方案是每个进程一个卡</em>，因此通常情况下world size实际为总的GPU数量或者总的进程数。</li>
<li>rank。表示当前进程或GPU的序号，用于进程间通讯。从0开始排序，范围是0~world size-1。注意：rank=0表示该进程是master进程，通常用来存储模型日志等。</li>
</ul>
<p>此外程序中还可能出现node_rank以及local_rank的概念：</p>
<ul>
<li>node_rank。表示当前机器的序号，同样也是从0开始排序，假设一共使用了N台机器，则node_rank的范围是0～N-1。</li>
<li>local_rank。表示当前机器上的进程或GPU的序号，从0开始排序，假设当前机器可使用GPU数量是N，则local_rank范围是0～N-1。</li>
</ul>
<h3 id="ddp_2">DDP的工作流程</h3>
<p>Pytorch DDP的主要工作流程：</p>
<ol>
<li>启动各进程。</li>
<li>各进程通过指定的init method完成握手，以便进行进程之间的通信。</li>
<li>主节点载入或随机初始化模型，并且将参数和Buffer（Buffer是会被持久化保存的数据等，如BatchNorm中的mean和variance）等模型状态信息分发至各GPU节点，此时每个GPU节点拿到的模型状态是相同的。</li>
<li>通过sampler，<strong>每个GPU拿到了属于自己的那份数据</strong>，并进行前向运算，并且计算出loss。</li>
<li>每个GPU进行backward()后向运算求出梯度（<strong>此时每个GPU上的梯度是不同的</strong>），之后各GPU通过AllReduce算法对梯度进行同步，使得每个GPU都能拿到全部GPU上的梯度的和（<strong>这时每个GPU上的梯度是相同的</strong>）。</li>
<li>各GPU根据同步之后的梯度来更新模型参数，更新之后各个GPU上的模型参数是相同的。</li>
<li>重复执行4~6，直至训练完毕。</li>
</ol>
<h3 id="wenet_4">WeNet分布式多机训练实现</h3>
<p>首先多机分布式训练通常需要分布式存储系统的支持，如S3、HDFS和NFS。由于PyTorch官方未像TensorFlow一样提供S3和HDFS的接口，因此可以使用NFS作为分布式存储。WeNet首先进行进程初始化，用于完成各进程握手的<code>init_process_group()</code>函数，需要传递的变量有backend，init_method，world_size和rank等。注意：在使用多机多卡训练时，<strong>先启动的机器节点上的进程会在此等待，直到所有进程都执行到这里</strong>，此时总的进程数与world_size相同，才会继续执行。<em>所以当world_size值和实际的总进程数不一致时，会产生Bug：先启动的进程会一直等待，直至超时退出</em>。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-46-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-46-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-46-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-46-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-46-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-46-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-46-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-46-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-46-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-46-10">10</a></span>
<span class="normal"><a href="#__codelineno-46-11">11</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-46-1" name="__codelineno-46-1"></a><span class="c1"># wenet/bin/train.py</span>
<a id="__codelineno-46-2" name="__codelineno-46-2"></a><span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">dist_backend</span><span class="p">,</span>
<a id="__codelineno-46-3" name="__codelineno-46-3"></a>                        <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">init_method</span><span class="p">,</span>
<a id="__codelineno-46-4" name="__codelineno-46-4"></a>                        <span class="n">world_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span>
<a id="__codelineno-46-5" name="__codelineno-46-5"></a>                        <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span>
<a id="__codelineno-46-6" name="__codelineno-46-6"></a><span class="o">...</span>    
<a id="__codelineno-46-7" name="__codelineno-46-7"></a><span class="n">train_data_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span>
<a id="__codelineno-46-8" name="__codelineno-46-8"></a>                                <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-46-9" name="__codelineno-46-9"></a>                                <span class="n">pin_memory</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">,</span>
<a id="__codelineno-46-10" name="__codelineno-46-10"></a>                                <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span>
<a id="__codelineno-46-11" name="__codelineno-46-11"></a>                                <span class="n">prefetch_factor</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">prefetch</span><span class="p">)</span>  
</code></pre></div></td></tr></table></div>
<p>之后通过DistributedDataParall将模型的状态信息从master进程节点传到其他进程节点，使所有进程上的模型状态一致。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-47-1">1</a></span>
<span class="normal"><a href="#__codelineno-47-2">2</a></span>
<span class="normal"><a href="#__codelineno-47-3">3</a></span>
<span class="normal"><a href="#__codelineno-47-4">4</a></span>
<span class="normal"><a href="#__codelineno-47-5">5</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-47-1" name="__codelineno-47-1"></a><span class="c1"># wenet/bin/train.py</span>
<a id="__codelineno-47-2" name="__codelineno-47-2"></a><span class="c1"># cuda model is required for nn.parallel.DistributedDataParallel</span>
<a id="__codelineno-47-3" name="__codelineno-47-3"></a><span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<a id="__codelineno-47-4" name="__codelineno-47-4"></a><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span>
<a id="__codelineno-47-5" name="__codelineno-47-5"></a>    <span class="n">model</span><span class="p">,</span> <span class="n">find_unused_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>之后的流程便和正常的训练一致，但是需要额外注意，即<code>Gradient Accumulation</code>梯度累积机制，这个机制指每隔若干个batch step进行一次参数的更新，从而可以去模拟更大的batch size，使训练更加稳定。具体实现如下：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-48-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-48-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-48-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-48-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-48-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-48-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-48-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-48-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-48-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-48-10">10</a></span>
<span class="normal"><a href="#__codelineno-48-11">11</a></span>
<span class="normal"><a href="#__codelineno-48-12">12</a></span>
<span class="normal"><a href="#__codelineno-48-13">13</a></span>
<span class="normal"><a href="#__codelineno-48-14">14</a></span>
<span class="normal"><a href="#__codelineno-48-15">15</a></span>
<span class="normal"><a href="#__codelineno-48-16">16</a></span>
<span class="normal"><a href="#__codelineno-48-17">17</a></span>
<span class="normal"><a href="#__codelineno-48-18">18</a></span>
<span class="normal"><a href="#__codelineno-48-19">19</a></span>
<span class="normal"><a href="#__codelineno-48-20">20</a></span>
<span class="normal"><a href="#__codelineno-48-21">21</a></span>
<span class="normal"><a href="#__codelineno-48-22">22</a></span>
<span class="normal"><a href="#__codelineno-48-23">23</a></span>
<span class="normal"><a href="#__codelineno-48-24">24</a></span>
<span class="normal"><a href="#__codelineno-48-25">25</a></span>
<span class="normal"><a href="#__codelineno-48-26">26</a></span>
<span class="normal"><a href="#__codelineno-48-27">27</a></span>
<span class="normal"><a href="#__codelineno-48-28">28</a></span>
<span class="normal"><a href="#__codelineno-48-29">29</a></span>
<span class="normal"><a href="#__codelineno-48-30">30</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-48-1" name="__codelineno-48-1"></a><span class="c1"># wenet/utils/executors.py</span>
<a id="__codelineno-48-2" name="__codelineno-48-2"></a>
<a id="__codelineno-48-3" name="__codelineno-48-3"></a><span class="n">loss</span><span class="p">,</span> <span class="n">loss_att</span><span class="p">,</span> <span class="n">loss_ctc</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
<a id="__codelineno-48-4" name="__codelineno-48-4"></a>    <span class="n">feats</span><span class="p">,</span> <span class="n">feats_lengths</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
<a id="__codelineno-48-5" name="__codelineno-48-5"></a><span class="o">...</span>
<a id="__codelineno-48-6" name="__codelineno-48-6"></a><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<a id="__codelineno-48-7" name="__codelineno-48-7"></a><span class="o">...</span>
<a id="__codelineno-48-8" name="__codelineno-48-8"></a><span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="n">accum_grad</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-48-9" name="__codelineno-48-9"></a>    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">writer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-48-10" name="__codelineno-48-10"></a>        <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;train_loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">)</span>
<a id="__codelineno-48-11" name="__codelineno-48-11"></a>    <span class="c1"># Use mixed precision training</span>
<a id="__codelineno-48-12" name="__codelineno-48-12"></a>    <span class="k">if</span> <span class="n">use_amp</span><span class="p">:</span>
<a id="__codelineno-48-13" name="__codelineno-48-13"></a>        <span class="n">scaler</span><span class="o">.</span><span class="n">unscale_</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<a id="__codelineno-48-14" name="__codelineno-48-14"></a>        <span class="n">grad_norm</span> <span class="o">=</span> <span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip</span><span class="p">)</span>
<a id="__codelineno-48-15" name="__codelineno-48-15"></a>        <span class="c1"># Must invoke scaler.update() if unscale_() is used in</span>
<a id="__codelineno-48-16" name="__codelineno-48-16"></a>        <span class="c1"># the iteration to avoid the following error:</span>
<a id="__codelineno-48-17" name="__codelineno-48-17"></a>        <span class="c1">#   RuntimeError: unscale_() has already been called</span>
<a id="__codelineno-48-18" name="__codelineno-48-18"></a>        <span class="c1">#   on this optimizer since the last update().</span>
<a id="__codelineno-48-19" name="__codelineno-48-19"></a>        <span class="c1"># We don&#39;t check grad here since that if the gradient</span>
<a id="__codelineno-48-20" name="__codelineno-48-20"></a>        <span class="c1"># has inf/nan values, scaler.step will skip</span>
<a id="__codelineno-48-21" name="__codelineno-48-21"></a>        <span class="c1"># optimizer.step().</span>
<a id="__codelineno-48-22" name="__codelineno-48-22"></a>        <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<a id="__codelineno-48-23" name="__codelineno-48-23"></a>        <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
<a id="__codelineno-48-24" name="__codelineno-48-24"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-48-25" name="__codelineno-48-25"></a>        <span class="n">grad_norm</span> <span class="o">=</span> <span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip</span><span class="p">)</span>
<a id="__codelineno-48-26" name="__codelineno-48-26"></a>        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">grad_norm</span><span class="p">):</span>
<a id="__codelineno-48-27" name="__codelineno-48-27"></a>            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<a id="__codelineno-48-28" name="__codelineno-48-28"></a>    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<a id="__codelineno-48-29" name="__codelineno-48-29"></a>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<a id="__codelineno-48-30" name="__codelineno-48-30"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></td></tr></table></div>
<p>但是如上的实现存在一个性能问题。如上文所述，梯度的AllReduce操作是在<code>backward()</code>时进行，因此<em>每个batch step都会进行各GPU进程之间梯度同步的操作</em>。由于Gradient Accumulation中每n个step才更新一次网络参数，因此其中n-1次<code>backward()</code>时进行的梯度同步结果不会被使用。而这些无用的梯度同步会增加分布式多机多卡训练时各机器节点的网络带宽压力。PyTorch最新的版本中支持的no_sync()上下文管理可以解决这个问题，如下：</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-49-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-49-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-49-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-49-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-49-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-49-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-49-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-49-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-49-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-49-10">10</a></span>
<span class="normal"><a href="#__codelineno-49-11">11</a></span>
<span class="normal"><a href="#__codelineno-49-12">12</a></span>
<span class="normal"><a href="#__codelineno-49-13">13</a></span>
<span class="normal"><a href="#__codelineno-49-14">14</a></span>
<span class="normal"><a href="#__codelineno-49-15">15</a></span>
<span class="normal"><a href="#__codelineno-49-16">16</a></span>
<span class="normal"><a href="#__codelineno-49-17">17</a></span>
<span class="normal"><a href="#__codelineno-49-18">18</a></span>
<span class="normal"><a href="#__codelineno-49-19">19</a></span>
<span class="normal"><a href="#__codelineno-49-20">20</a></span>
<span class="normal"><a href="#__codelineno-49-21">21</a></span>
<span class="normal"><a href="#__codelineno-49-22">22</a></span>
<span class="normal"><a href="#__codelineno-49-23">23</a></span>
<span class="normal"><a href="#__codelineno-49-24">24</a></span>
<span class="normal"><a href="#__codelineno-49-25">25</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-49-1" name="__codelineno-49-1"></a><span class="c1"># wenet/utils/executors.py</span>
<a id="__codelineno-49-2" name="__codelineno-49-2"></a>
<a id="__codelineno-49-3" name="__codelineno-49-3"></a><span class="c1"># Disable gradient synchronizations across DDP processes.</span>
<a id="__codelineno-49-4" name="__codelineno-49-4"></a><span class="c1"># Within this context, gradients will be accumulated on module</span>
<a id="__codelineno-49-5" name="__codelineno-49-5"></a><span class="c1"># variables, which will later be synchronized.</span>
<a id="__codelineno-49-6" name="__codelineno-49-6"></a><span class="k">if</span> <span class="n">is_distributed</span> <span class="ow">and</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="n">accum_grad</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-49-7" name="__codelineno-49-7"></a>    <span class="n">context</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">no_sync</span>
<a id="__codelineno-49-8" name="__codelineno-49-8"></a><span class="c1"># Used for single gpu training and DDP gradient synchronization</span>
<a id="__codelineno-49-9" name="__codelineno-49-9"></a><span class="c1"># processes.</span>
<a id="__codelineno-49-10" name="__codelineno-49-10"></a><span class="k">else</span><span class="p">:</span>
<a id="__codelineno-49-11" name="__codelineno-49-11"></a>    <span class="n">context</span> <span class="o">=</span> <span class="n">nullcontext</span>
<a id="__codelineno-49-12" name="__codelineno-49-12"></a>
<a id="__codelineno-49-13" name="__codelineno-49-13"></a><span class="c1"># 如果在model.no_sync()上下文中，不会同步各个节点间的梯度</span>
<a id="__codelineno-49-14" name="__codelineno-49-14"></a><span class="k">with</span> <span class="n">context</span><span class="p">():</span>
<a id="__codelineno-49-15" name="__codelineno-49-15"></a>    <span class="c1"># autocast context</span>
<a id="__codelineno-49-16" name="__codelineno-49-16"></a>    <span class="c1"># The more details about amp can be found in</span>
<a id="__codelineno-49-17" name="__codelineno-49-17"></a>    <span class="c1"># https://pytorch.org/docs/stable/notes/amp_examples.html</span>
<a id="__codelineno-49-18" name="__codelineno-49-18"></a>    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">scaler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
<a id="__codelineno-49-19" name="__codelineno-49-19"></a>        <span class="n">loss</span><span class="p">,</span> <span class="n">loss_att</span><span class="p">,</span> <span class="n">loss_ctc</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
<a id="__codelineno-49-20" name="__codelineno-49-20"></a>            <span class="n">feats</span><span class="p">,</span> <span class="n">feats_lengths</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
<a id="__codelineno-49-21" name="__codelineno-49-21"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">accum_grad</span>
<a id="__codelineno-49-22" name="__codelineno-49-22"></a>    <span class="k">if</span> <span class="n">use_amp</span><span class="p">:</span>
<a id="__codelineno-49-23" name="__codelineno-49-23"></a>        <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<a id="__codelineno-49-24" name="__codelineno-49-24"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-49-25" name="__codelineno-49-25"></a>        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<p>累计梯度时使用no_sync上下文，这时<code>backward()</code>将不再进行梯度的同步。</p>
<h3 id="wenet_5">WeNet分布式使用实践</h3>
<p>首先需要确定总共使用的机器数num_nodes，如果是单机训练，则设置num_nodes=1，node_rank=0即可。如果使用多机训练模式，则需要先指定机器节点的数量num_nodes，然后<strong>在每个机器节点启动脚本中指定该节点的node_rank</strong>。例如一共使用了两台机器，则设置num_nodes=2，首先在第一个节点启动<code>run.sh</code>脚本并设置node_rank=0，其次在第二个节点启动<code>run.sh</code>脚本并设置node_rank=1。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-50-1">1</a></span>
<span class="normal"><a href="#__codelineno-50-2">2</a></span>
<span class="normal"><a href="#__codelineno-50-3">3</a></span>
<span class="normal"><a href="#__codelineno-50-4">4</a></span>
<span class="normal"><a href="#__codelineno-50-5">5</a></span>
<span class="normal"><a href="#__codelineno-50-6">6</a></span>
<span class="normal"><a href="#__codelineno-50-7">7</a></span>
<span class="normal"><a href="#__codelineno-50-8">8</a></span>
<span class="normal"><a href="#__codelineno-50-9">9</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-50-1" name="__codelineno-50-1"></a><span class="c1"># wenet/examples/aishell/s0/run.sh</span>
<a id="__codelineno-50-2" name="__codelineno-50-2"></a>
<a id="__codelineno-50-3" name="__codelineno-50-3"></a><span class="c1"># The num of machines(nodes) for multi-machine training, 1 is for one machine.</span>
<a id="__codelineno-50-4" name="__codelineno-50-4"></a><span class="c1"># NFS is required if num_nodes &gt; 1.</span>
<a id="__codelineno-50-5" name="__codelineno-50-5"></a><span class="nv">num_nodes</span><span class="o">=</span><span class="m">1</span>
<a id="__codelineno-50-6" name="__codelineno-50-6"></a><span class="c1"># The rank of each node or machine, which ranges from 0 to `num_nodes - 1`.</span>
<a id="__codelineno-50-7" name="__codelineno-50-7"></a><span class="c1"># You should set the node_rank=0 on the first machine, set the node_rank=1</span>
<a id="__codelineno-50-8" name="__codelineno-50-8"></a><span class="c1"># on the second machine, and so on.</span>
<a id="__codelineno-50-9" name="__codelineno-50-9"></a><span class="nv">node_rank</span><span class="o">=</span><span class="m">0</span>
</code></pre></div></td></tr></table></div>
<p>每台机器在启动<code>run.sh</code>脚本后，将会在其所在的主机上启动包含的GPU数量相同的进程，并计算各GPU或进程的rank，从而完成多机多卡的分布式训练。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-51-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-51-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-51-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-51-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-51-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-51-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-51-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-51-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-51-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-51-10">10</a></span>
<span class="normal"><a href="#__codelineno-51-11">11</a></span>
<span class="normal"><a href="#__codelineno-51-12">12</a></span>
<span class="normal"><a href="#__codelineno-51-13">13</a></span>
<span class="normal"><a href="#__codelineno-51-14">14</a></span>
<span class="normal"><a href="#__codelineno-51-15">15</a></span>
<span class="normal"><a href="#__codelineno-51-16">16</a></span>
<span class="normal"><a href="#__codelineno-51-17">17</a></span>
<span class="normal"><a href="#__codelineno-51-18">18</a></span>
<span class="normal"><a href="#__codelineno-51-19">19</a></span>
<span class="normal"><a href="#__codelineno-51-20">20</a></span>
<span class="normal"><a href="#__codelineno-51-21">21</a></span>
<span class="normal"><a href="#__codelineno-51-22">22</a></span>
<span class="normal"><a href="#__codelineno-51-23">23</a></span>
<span class="normal"><a href="#__codelineno-51-24">24</a></span>
<span class="normal"><a href="#__codelineno-51-25">25</a></span>
<span class="normal"><a href="#__codelineno-51-26">26</a></span>
<span class="normal"><a href="#__codelineno-51-27">27</a></span>
<span class="normal"><a href="#__codelineno-51-28">28</a></span>
<span class="normal"><a href="#__codelineno-51-29">29</a></span>
<span class="normal"><a href="#__codelineno-51-30">30</a></span>
<span class="normal"><a href="#__codelineno-51-31">31</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-51-1" name="__codelineno-51-1"></a><span class="c1"># wenet/examples/aishell/s0/run.sh  </span>
<a id="__codelineno-51-2" name="__codelineno-51-2"></a>
<a id="__codelineno-51-3" name="__codelineno-51-3"></a><span class="nv">num_gpus</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span> <span class="nv">$CUDA_VISIBLE_DEVICES</span> <span class="p">|</span> awk -F <span class="s2">&quot;,&quot;</span> <span class="s1">&#39;{print NF}&#39;</span><span class="k">)</span>
<a id="__codelineno-51-4" name="__codelineno-51-4"></a>...
<a id="__codelineno-51-5" name="__codelineno-51-5"></a><span class="c1"># train.py rewrite $train_config to $dir/train.yaml with model input</span>
<a id="__codelineno-51-6" name="__codelineno-51-6"></a><span class="c1"># and output dimension, and $dir/train.yaml will be used for inference</span>
<a id="__codelineno-51-7" name="__codelineno-51-7"></a><span class="c1"># and export.</span>
<a id="__codelineno-51-8" name="__codelineno-51-8"></a><span class="k">for</span> <span class="o">((</span><span class="nv">i</span> <span class="o">=</span> <span class="m">0</span><span class="p">;</span> i &lt; <span class="nv">$num_gpus</span><span class="p">;</span> ++i<span class="o">))</span><span class="p">;</span> <span class="k">do</span>
<a id="__codelineno-51-9" name="__codelineno-51-9"></a><span class="o">{</span>
<a id="__codelineno-51-10" name="__codelineno-51-10"></a><span class="nv">gpu_id</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span> <span class="nv">$CUDA_VISIBLE_DEVICES</span> <span class="p">|</span> cut -d<span class="s1">&#39;,&#39;</span> -f$<span class="o">[</span><span class="nv">$i</span>+1<span class="o">]</span><span class="k">)</span>
<a id="__codelineno-51-11" name="__codelineno-51-11"></a><span class="c1"># Rank of each gpu/process used for knowing whether it is</span>
<a id="__codelineno-51-12" name="__codelineno-51-12"></a><span class="c1"># the master of a worker.</span>
<a id="__codelineno-51-13" name="__codelineno-51-13"></a><span class="nv">rank</span><span class="o">=</span><span class="sb">`</span>expr <span class="nv">$node_rank</span> <span class="se">\*</span> <span class="nv">$num_gpus</span> + <span class="nv">$i</span><span class="sb">`</span>
<a id="__codelineno-51-14" name="__codelineno-51-14"></a>python wenet/bin/train.py --gpu <span class="nv">$gpu_id</span> <span class="se">\</span>
<a id="__codelineno-51-15" name="__codelineno-51-15"></a>    --config <span class="nv">$train_config</span> <span class="se">\</span>
<a id="__codelineno-51-16" name="__codelineno-51-16"></a>    --data_type <span class="nv">$data_type</span> <span class="se">\</span>
<a id="__codelineno-51-17" name="__codelineno-51-17"></a>    --symbol_table <span class="nv">$dict</span> <span class="se">\</span>
<a id="__codelineno-51-18" name="__codelineno-51-18"></a>    --train_data data/<span class="nv">$train_set</span>/data.list <span class="se">\</span>
<a id="__codelineno-51-19" name="__codelineno-51-19"></a>    --cv_data data/dev/data.list <span class="se">\</span>
<a id="__codelineno-51-20" name="__codelineno-51-20"></a>    <span class="si">${</span><span class="nv">checkpoint</span><span class="p">:+--checkpoint </span><span class="nv">$checkpoint</span><span class="si">}</span> <span class="se">\</span>
<a id="__codelineno-51-21" name="__codelineno-51-21"></a>    --model_dir <span class="nv">$dir</span> <span class="se">\</span>
<a id="__codelineno-51-22" name="__codelineno-51-22"></a>    --ddp.init_method <span class="nv">$init_method</span> <span class="se">\</span>
<a id="__codelineno-51-23" name="__codelineno-51-23"></a>    --ddp.world_size <span class="nv">$world_size</span> <span class="se">\</span>
<a id="__codelineno-51-24" name="__codelineno-51-24"></a>    --ddp.rank <span class="nv">$rank</span> <span class="se">\</span>
<a id="__codelineno-51-25" name="__codelineno-51-25"></a>    --ddp.dist_backend <span class="nv">$dist_backend</span> <span class="se">\</span>
<a id="__codelineno-51-26" name="__codelineno-51-26"></a>    --num_workers <span class="m">1</span> <span class="se">\</span>
<a id="__codelineno-51-27" name="__codelineno-51-27"></a>    <span class="nv">$cmvn_opts</span> <span class="se">\</span>
<a id="__codelineno-51-28" name="__codelineno-51-28"></a>    --pin_memory
<a id="__codelineno-51-29" name="__codelineno-51-29"></a><span class="o">}</span> <span class="p">&amp;</span>
<a id="__codelineno-51-30" name="__codelineno-51-30"></a><span class="k">done</span>
<a id="__codelineno-51-31" name="__codelineno-51-31"></a><span class="nb">wait</span>
</code></pre></div></td></tr></table></div>
<h3 id="wenet_6">WeNet实验结果</h3>
<p>Reference：<a href="https://zhuanlan.zhihu.com/p/355527000">WeNet更新：支持多机并行训练</a></p>
<h4 id="no_sync">no_sync上下文</h4>
<p>通过监控各节点的网络流量情况来验证no_sync上下文是否生效。如下是采用累计梯度为4，同一机器节点上网卡的流量情况。左侧为未使用no_sync上下文，右图为使用了no_sync上下文。</p>
<p><img alt="" src="../vx_images/171184418268674.png" /></p>
<p>可以看到，no_sync上下文极大的降低了网络流量，减小了对网络带宽的需求压力。</p>
<h4 id="_12">网络带宽对多机训练加速比的影响</h4>
<p>如上所述，虽然DDP采用的AllReduce的梯度同步方案，各GPU通信时处于平等的状态，平衡了各节点的通信流量，但是<strong>DDP仍然需要较好的网络带宽，才能达到更好的多机线性加速</strong>，各机器节点之间通信的网络带宽会成为系统的瓶颈。当网络带宽过小时，在进行梯度同步时，由于带宽过小会导致延迟大，梯度同步通信时出现等待的现象。下表记录了WeNet使用了两台机器一共16块GPU，累计梯度为4的情况下网络带宽对训练速度的影响，使用的语料为AISHELL-1。</p>
<table>
<thead>
<tr>
<th>训练配置</th>
<th>每个epoch训练时间</th>
</tr>
</thead>
<tbody>
<tr>
<td>单机8块GPU训练</td>
<td>6分50秒</td>
</tr>
<tr>
<td>多机16块GPU&amp;千兆网卡</td>
<td>22分</td>
</tr>
<tr>
<td>多机16块GPU&amp;万兆网卡</td>
<td>3分48秒</td>
</tr>
</tbody>
</table>
<p>可以看到<em>当使用千兆网时，两台机器16卡训练的速度甚至比单机8卡还要慢很多</em>，改成万兆网卡之后便可得到不错的加速。由此可见，<strong>DDP也需要较好的网络带宽的支持，防止在做梯度更新时，网卡的带宽成为系统的瓶颈</strong>。</p>
<h4 id="no_sync_1">no_sync上下文的影响</h4>
<p>上下文对每个epoch的训练时间影响如下。</p>
<table>
<thead>
<tr>
<th>训练配置</th>
<th>no_sync上下文</th>
<th>每个epoch训练时间</th>
</tr>
</thead>
<tbody>
<tr>
<td>单机8块GPU训练</td>
<td>采用</td>
<td>6分50秒</td>
</tr>
<tr>
<td>多机16块GPU&amp;千兆网卡</td>
<td>不采用</td>
<td>22分</td>
</tr>
<tr>
<td>多机16块GPU&amp;千兆网卡</td>
<td>采用</td>
<td>8分10秒</td>
</tr>
<tr>
<td>多机16块GPU&amp;万兆网卡</td>
<td>不采用</td>
<td>3分48秒</td>
</tr>
<tr>
<td>多机16块GPU&amp;万兆网卡</td>
<td>采用</td>
<td>3分35秒</td>
</tr>
</tbody>
</table>
<p>通过对比我们可以发现，不管是千兆网还是万兆网，使用no_sync上下文都比未使用no_sync上下文更快了些。<strong>万兆网卡+梯度no_sync上下文后几乎接近较为完美的线性加速比</strong>，并且从数据上看，万兆网卡是决定性因素。</p>
<h4 id="_13">多机多卡的模型效果对比</h4>
<p>单机训练与多机训练最终模型的CER的对比如下。</p>
<table>
<thead>
<tr>
<th>解码方式</th>
<th>单机4卡</th>
<th>多机16卡</th>
</tr>
</thead>
<tbody>
<tr>
<td>attention decoder</td>
<td>5.18</td>
<td>4.90</td>
</tr>
<tr>
<td>ctc greedy search</td>
<td>4.94</td>
<td>5.07</td>
</tr>
<tr>
<td>ctc prefix beam serach</td>
<td>4.94</td>
<td>5.06</td>
</tr>
<tr>
<td>attention rescoring</td>
<td>4.61</td>
<td>4.65</td>
</tr>
</tbody>
</table>
<p>通过对比可以看到，<strong>多机训练的识别性能与单机训练的识别性能基本一致</strong>。在模型效果几乎不变的情况下，WeNet多机训练近乎达到了完美的线性加速。参见wenet/examples/aishell/s0/README.md</p>
<h4 id="nfs">基于NFS多机训练</h4>
<p>NFS是一种便于数据共享的服务，但并不适用于多机分布式高性能计算。最主要的原因是在进行多机训练时，从client节点会不断的访问存放数据的server节点，此时<strong>server节点的网络和磁盘带宽会成为系统的瓶颈</strong>。这里有一个小trick可以缓解这个问题：<strong>就是将要访问的数据的存储分散至多个server节点上</strong>，这样同一时刻，所有进程想要访问的数据会向不同的NFS的server节点访问，这样极大的减小了单一server节点时的网络和磁盘带宽问题。</p>
<h3 id="_14">总结</h3>
<ol>
<li>万兆网卡，是保证多机训练线性加速的关键。</li>
<li>no_sync与acc_grad的使用可以极大降低多机训练时的流量，减小系统对带宽的需求，可以得到更好的加速比。</li>
<li>基于NFS的多机存储方案，server点的带宽会成为系统瓶颈，为此可以将文件分散至不同节点上，负载均衡各节点网络流量。</li>
</ol>
<p><strong>网络带宽对于分布式训练来说至关重要</strong>。除上述的方法，还可以使用amp或apex等软件包提供的<strong>混合精度训练</strong>，这样在梯度同步时，使用fp16来进行同步，相较于现在fp32可减少一倍的网络流量。</p>

  <hr>
<div class="md-source-file">
  <small>
    
      最后更新:
      2022-06-08
    
  </small>
</div>

              

  <!-- Giscus -->
  <h2 id="__comments">评论</h2>
  <!-- Replace with generated snippet -->
  <script src="https://giscus.app/client.js"
        data-repo="cnlinxi/blog"
        data-repo-id="R_kgDOHLI9Jw"
        data-category="Announcements"
        data-category-id="DIC_kwDOHLI9J84COkTs"
        data-mapping="pathname"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="light"
        data-lang="zh-CN"
        crossorigin="anonymous"
        async>
  </script>

  <!-- Reload on palette change -->
  <script>
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object")
      if (palette.color.scheme === "slate") {
        var giscus = document.querySelector("script[src*=giscus]")
        giscus.setAttribute("data-theme", "dark") 
      }

    /* Register event handlers after documented loaded */
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate" ? "dark" : "light"

          /* Instruct Giscus to change theme */
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script>

            </article>
            
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            回到页面顶部
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="页脚" >
      
        
        <a href="../OpenFST%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/" class="md-footer__link md-footer__link--prev" aria-label="上一页: OpenFST基本操作" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              OpenFST基本操作
            </div>
          </div>
        </a>
      
      
        
        <a href="../wenet_runtime%E7%9F%A5%E8%AF%86%E7%82%B9/" class="md-footer__link md-footer__link--next" aria-label="下一页: WeNet runtime知识点" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                下一页
              </span>
              WeNet runtime知识点
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.expand", "toc.follow", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "search.share"], "search": "../../assets/javascripts/workers/search.b028fd86.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\s\\-\uff0c\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f758a944.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>