# 语音合成的评价标准和方法

## 语音合成评价指标

对合成语音的质量评价，主要可以分为主观和客观评价。主观评价是通过人类对语音进行打分，比如平均意见得分（Mean
Opinion Score，MOS）、众包平均意见得分（CrowdMOS，CMOS）和ABX测试。客观评价是通过计算机自动给出语音音质的评估，在语音合成领域研究的比较少，论文中常常通过展示频谱细节，计算梅尔倒谱失真（Mel Cepstral Distortion，MCD）等方法作为客观评价。客观评价还可以分为有参考和无参考质量评估，这两者的主要判别依据在于该方法是否需要标准信号。有参考评估方法除了待评测信号，还需要一个音质优异的，可以认为没有损伤的参考信号。常见的有参考质量评估主要有ITU-T P.861 (MNB)、ITU-T P.862 (PESQ)、ITU-T P.863 (POLQA)、STOI和BSSEval。无参考评估方法则不需要参考信号，直接根据待评估信号，给出质量评分，无参考评估方法还可以分为基于信号、基于参数以及基于深度学习的质量评估方法。常见的基于信号的无参考质量评估包括ITU-T P.563和ANIQUE+，基于参数的方法有ITU-T G.107(E-Model)。近年来，深度学习也逐步应用到无参考质量评估中，如：AutoMOS、QualityNet、NISQA和MOSNet。

主观评价中的MOS评测是一种较为宽泛的说法，由于给出评测分数的主体是人类，因此可以灵活测试语音的不同方面。比如在语音合成领域，主要有自然度MOS（MOS of Naturalness）和相似度MOS（MOS of Similarity）。但是人类给出的评分结果受到的干扰因素较多，谷歌对合成语音的主观评估方法进行了比较，在评估较长语音中的单个句子时，音频样本的呈现形式会显著影响参与人员给出的结果。比如仅提供单个句子而不提供上下文，与相同句子给出语境相比，被测人员给出的评分差异显著。国际电信联盟（International Telecommunication Union，ITU）将MOS评测规范化为ITU-T P.800，其中绝对等级评分（Absolute Category Rating，ACR）应用最为广泛，ACR的详细评估标准如下表所示。

  |  音频级别  |  平均意见得分 |  评价标准    |                                  
  |:---:|:----:|:------------------------|
  | 优  |        5.0       |     很好，听得清楚；延迟小，交流流畅             |
  | 良  |        4.0       |     稍差，听得清楚；延迟小，交流欠流畅，有点杂音   |
  | 中  |        3.0       |     还可以，听不太清；有一定延迟，可以交流        |
  | 差  |        2.0       |     勉强，听不太清；延迟较大，交流需要重复多遍     |
  | 劣  |        1.0       |     极差，听不懂；延迟大，交流不通畅             |

在使用ACR方法对语音质量进行评价时，参与评测的人员（简称被试）对语音整体质量进行打分，分值范围为1 5分，分数越大表示语音质量越好。MOS大于4时，可以认为该音质受到大部分被试的认可，音质较好；若MOS低于3，则该语音有比较大的缺陷，大部分被试并不满意该音质。

## 平均意见得分的测评要求与方法

语音合成的最终目标是，合成语音应尽可能接近真实发音，以至于人类无法区分合成和真实语音。因此让人类对合成语音进行评价打分是最为直观的评价方法，评分经处理之后即可获得平均意见得分。平均意见得分是语音合成系统最重要的性能指标之一，能够直接反映合成语音的自然度、清晰度以及可懂度。

### 实验要求

获取多样化且数量足够大的音频样本，以确保结果在统计上的显著，测评在具有特定声学特性的设备上进行，控制每个被试遵循同样的评估标准，并且确保每个被试的实验环境保持一致。

### 实验方法

为了达到实验要求，可以通过两种方法获得足够精确的测评结果。第一种是实验室方式，该方式让被试在实验室环境中进行测评，在试听过程中环境噪音必须低于35dB，测试语音数量至少保持30个以上，且覆盖该语种所有音素和音素组合，参与评测的被试应尽可能熟练掌握待测合成语音的语种，最好以合成语音的语种为母语。该方法的优点是测试要素容易控制，能够稳定保证实验环境达到测评要求；缺点则主要是需要被试在固定场所完成试听，人力成本高。第二种是众包，也就是将任务发布到网络上，让具有条件的被试在任何地方进行测评。该方法主要优点是易于获得较为有效的评估结果；而缺点则体现在无法确保试听条件。

### 实验步骤

1.  收集合成语音和录制的真实语音；

2.  确保文本和语音一一对应，去除发音明显错误的音频样本；

3.  生成问卷，将合成语音和真实语音交叉打乱，确保打乱的顺序没有规律，合成语音和真实语音不可让被试提前探知到；

4.  开始任务前，被试试听示例语音，并告知其对应的大致得分；

5.  被试开始对给定音频打分，前三条语音可以作为被试进入平稳打分状态的铺垫，不计入最终结果；

6.  回收问卷，舍弃有明显偏差的评价数据，统计最终得分。

### 实验设计

1.  准备测试语音数据。(1)从各领域和语音合成系统实际应用场景中，摘选常规文本作为测试语料，选取的语句一般尽可能排除生僻字；(2)用于测试的句子一般是未出现在训练集中的；(3)
    被试必须使用耳机试听语音，以便于判断更为细微的差别；(4)为了避免被试的疲惫，待测评系统和语料数量不可太多，需要控制测评时间；(5)一个句子需要由多个被试打分。

2.  设置实验参数。在准备测试语音时，需要提前设置好训练语料、待测系统、参与测试的句子数量、每个句子被试听的次数等。以中文语音合成系统的语音评估为例，测评设置如下表所示。

      |训练集      | 待测系统     |  句子数量  | 每个句子被测次数 |
      |:---------:|:------------:|:----------:|:------------:|
      | 内部数据集  | 真实语音      |  40        | 12           |
      | 内部数据集  | Tacotron-2   |   40       | 12           |
      | 内部数据集  | FastSpeech-2 |  40        | 12           |

3.  准备HTML文档等展示材料，向被试介绍该测试。该HTML文档至少包括：(1)测试注意事项，如被试应该使用何种设备，在何种环境下试听，试听时应该排除的干扰因素等；(2)测试任务，向被试介绍本次试听的测试目标，应关注的侧重点，如：可懂度、相似度、清晰度等方面；(3)参考音频，可以放置一些示例音频，如MOS=5的优质语音，MOS=1的低劣音频，以便被试更好地对音频打分；(4)
    测试音频，根据不同任务，放置合理的测试音频，真实和合成音频应提前打乱，并且不可告知被试打乱的顺序。

### 实验数据处理

1.  数据筛选。由于被试有可能没有受到监督，因此需要对收集到的评分进行事后检查，如删除使用扬声器试听的评分。另外，为了控制个体因素对整体结果的影响，减少偏离整体数据的异常值，需要计算每个人的评分与总体得分序列的相关性，相关性的度量使用相关系数来实现，如果相关系数r大于0.25，则保留；否则拒绝该被试的所有评分。相关系数r的计算方法如下：

$$
r=\frac{\mathop{cov}(\mu_{1n},...,\mu_{Mn};\mu_1,...,\mu_M)}{\sqrt{\mathop{var}(\mu_{1n},...,\mu_{Mn})}\cdot \sqrt{\mathop{var}(\mu_1,...,\mu_M)}}
$$

其中， $M$ 为句子数量，$N$为被试数量， $\mu_{mn}$ 为被试 $n$ 对句子 $m$ 给出的评分，$1\leq m\leq M$ ,$1\leq n\leq N$, $\mu_m=1/N\sum_{n=1}^N\mu_{mn}$ 为句子 $m$ 的总体平均分， $\mathop{cov}$为协方差， $\mathop{var}$ 为方差。